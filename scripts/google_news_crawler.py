#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Google æ–°é—»æœç´¢çˆ¬è™« - è·å–24å°æ—¶å†…çš„æœ€æ–°æ–°é—»
"""

from gnews import GNews
import json
import os
from datetime import datetime, timedelta

class GoogleNewsCrawler:
    """Google æ–°é—»æœç´¢çˆ¬è™«"""
    
    def __init__(self, sector, hours=24):
        self.sector = sector
        self.hours = hours
        self.results = []
        
        # åˆå§‹åŒ– GNews
        self.google_news = GNews(
            language='zh',  # ä¸­æ–‡
            country='CN',   # ä¸­å›½
            period=f'{hours}h',  # æ—¶é—´èŒƒå›´
            max_results=100  # æ¯ä¸ªå…³é”®è¯æœ€å¤š100æ¡
        )
        
        self.config = self.load_config()
    
    def load_config(self):
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        script_dir = os.path.dirname(os.path.abspath(__file__))
        config_path = os.path.join(script_dir, '../references/config.json')
        with open(config_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    
    def search_news(self, keyword):
        """
        æœç´¢æ–°é—»
        
        å‚æ•°:
            keyword: æœç´¢å…³é”®è¯
        """
        print(f"\nğŸ” æœç´¢ Google æ–°é—»: {keyword}")
        
        try:
            # æœç´¢æ–°é—»
            news_list = self.google_news.get_news(keyword)
            
            news_items = []
            for news in news_list:
                news_items.append({
                    'title': news.get('title', ''),
                    'url': news.get('url', ''),
                    'source': news.get('publisher', {}).get('title', 'æœªçŸ¥æ¥æº'),
                    'published_date': news.get('published date', ''),
                    'keyword': keyword,
                    'date': datetime.now().strftime('%Y-%m-%d')
                })
            
            print(f"  âœ“ æ‰¾åˆ° {len(news_items)} æ¡æ–°é—»")
            return news_items
            
        except Exception as e:
            print(f"  âœ— æœç´¢å¤±è´¥: {e}")
            return []
    
    def search_with_keywords(self, keywords):
        """ä½¿ç”¨å¤šä¸ªå…³é”®è¯æœç´¢"""
        print(f"\n{'='*60}")
        print(f"å¼€å§‹æœç´¢ Google æ–°é—» - {self.config['sectors'][self.sector]['name']}")
        print(f"æ—¶é—´èŒƒå›´: æœ€è¿‘ {self.hours} å°æ—¶")
        print(f"{'='*60}")
        
        for keyword in keywords:
            news_items = self.search_news(keyword)
            self.results.extend(news_items)
        
        # å»é‡
        self.deduplicate()
    
    def deduplicate(self):
        """å»é‡"""
        seen_titles = set()
        unique_results = []
        
        for news in self.results:
            title = news['title']
            if title not in seen_titles:
                seen_titles.add(title)
                unique_results.append(news)
        
        removed = len(self.results) - len(unique_results)
        if removed > 0:
            print(f"\nğŸ”„ å»é‡: ç§»é™¤ {removed} æ¡é‡å¤æ–°é—»")
        
        self.results = unique_results
    
    def save_results(self):
        """ä¿å­˜ç»“æœ"""
        if not self.results:
            print("\nâš ï¸  æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ–°é—»")
            return
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        script_dir = os.path.dirname(os.path.abspath(__file__))
        output_dir = os.path.join(script_dir, '../data/raw')
        os.makedirs(output_dir, exist_ok=True)
        
        # ä¿å­˜ä¸º JSON
        date_str = datetime.now().strftime('%Y%m%d')
        output_file = os.path.join(output_dir, f'{self.sector}_google_{date_str}.json')
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ å·²ä¿å­˜åˆ°: {output_file}")
        print(f"ğŸ“Š å…±ä¿å­˜: {len(self.results)} æ¡æ–°é—»")
        
        # æ‰“å°å‰5æ¡æ ‡é¢˜
        print(f"\nğŸ“° å‰5æ¡æ–°é—»æ ‡é¢˜ï¼š")
        for i, news in enumerate(self.results[:5], 1):
            print(f"{i}. {news['title']}")
            print(f"   æ¥æº: {news['source']} | å‘å¸ƒ: {news['published_date']}")

if __name__ == '__main__':
    import argparse
    
    parser = argparse.ArgumentParser(description='Google æ–°é—»æœç´¢çˆ¬è™«')
    parser.add_argument('--sector', required=True, 
                        choices=['healthcare', 'education', 'strategic_emerging', 'hightech'], 
                        help='æ¿å—: healthcare, education, strategic_emerging, hightech')
    parser.add_argument('--hours', type=int, default=24, help='æ—¶é—´èŒƒå›´ï¼ˆå°æ—¶ï¼‰')
    parser.add_argument('--keywords', nargs='+', help='æœç´¢å…³é”®è¯åˆ—è¡¨')
    
    args = parser.parse_args()
    
    crawler = GoogleNewsCrawler(args.sector, args.hours)
    
    # å¦‚æœæ²¡æœ‰æŒ‡å®šå…³é”®è¯ï¼Œä½¿ç”¨é»˜è®¤çš„ç¬¬ä¸€ä¼˜å…ˆçº§å…³é”®è¯
    if not args.keywords:
        if args.sector == 'healthcare':
            keywords = ["åŒ»è¯äº§ä¸š", "ç”Ÿç‰©åŒ»è¯", "åŒ»ç–—å¥åº·", "åŒ»ä¿æ”¹é©"]
        elif args.sector == 'education':
            keywords = ["äººæ‰æ”¿ç­–", "æ•™è‚²æ”¹é©", "äººæ‰åŸ¹å…»", "èŒä¸šæ•™è‚²"]
        elif args.sector == 'strategic_emerging':
            keywords = ["æˆ˜ç•¥æ–°å…´äº§ä¸š", "æ–°èƒ½æº", "æ–°ææ–™", "æ•°å­—ç»æµ"]
        elif args.sector == 'hightech':
            keywords = ["é«˜ç§‘æŠ€äº§ä¸š", "äººå·¥æ™ºèƒ½", "èŠ¯ç‰‡", "åŠå¯¼ä½“"]
    else:
        keywords = args.keywords
    
    crawler.search_with_keywords(keywords)
    crawler.save_results()

