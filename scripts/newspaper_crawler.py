#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Newspaper4k æ–°é—»æå–å™¨ - ä»æ–°é—»ç½‘ç«™æå–æ–‡ç« 
"""

from newspaper import Article, Source
import json
import os
from datetime import datetime, timedelta

class Newspaper4kCrawler:
    """Newspaper4k æ–°é—»æå–å™¨"""
    
    def __init__(self, sector, hours=24):
        self.sector = sector
        self.hours = hours
        self.results = []
        
        # æ–°é—»æºåˆ—è¡¨
        self.news_sources = {
            'healthcare': [
                'https://www.jkb.com.cn',  # å¥åº·æŠ¥
                'https://news.pharmnet.com.cn',  # åŒ»è¯ç½‘
            ],
            'education': [
                'http://edu.people.com.cn',  # äººæ°‘ç½‘æ•™è‚²
                'https://news.sciencenet.cn',  # ç§‘å­¦ç½‘
            ]
        }
    
    def crawl_source(self, source_url):
        """çˆ¬å–æ–°é—»æº"""
        print(f"\nğŸ” çˆ¬å–æ–°é—»æº: {source_url}")
        
        try:
            # æ„å»ºæ–°é—»æº
            source = Source(source_url, language='zh')
            source.build()
            
            news_items = []
            cutoff_time = datetime.now() - timedelta(hours=self.hours)
            
            # éå†æ–‡ç« 
            for article in source.articles[:50]:  # é™åˆ¶50ç¯‡
                try:
                    article.download()
                    article.parse()
                    
                    # æ£€æŸ¥å‘å¸ƒæ—¶é—´
                    if article.publish_date:
                        if article.publish_date.replace(tzinfo=None) < cutoff_time:
                            continue
                    
                    news_items.append({
                        'title': article.title,
                        'url': article.url,
                        'source': source_url,
                        'publish_date': str(article.publish_date) if article.publish_date else '',
                        'date': datetime.now().strftime('%Y-%m-%d')
                    })
                    
                except Exception as e:
                    continue
            
            print(f"  âœ“ æ‰¾åˆ° {len(news_items)} æ¡æ–°é—»")
            return news_items
            
        except Exception as e:
            print(f"  âœ— çˆ¬å–å¤±è´¥: {e}")
            return []
    
    def crawl_all_sources(self):
        """çˆ¬å–æ‰€æœ‰æ–°é—»æº"""
        print(f"\n{'='*60}")
        print(f"å¼€å§‹çˆ¬å– Newspaper4k æ–°é—»æº")
        print(f"{'='*60}")
        
        sources = self.news_sources.get(self.sector, [])
        
        for source_url in sources:
            news_items = self.crawl_source(source_url)
            self.results.extend(news_items)
        
        # å»é‡
        self.deduplicate()
    
    def deduplicate(self):
        """å»é‡"""
        seen_titles = set()
        unique_results = []
        
        for news in self.results:
            title = news['title']
            if title not in seen_titles:
                seen_titles.add(title)
                unique_results.append(news)
        
        removed = len(self.results) - len(unique_results)
        if removed > 0:
            print(f"\nğŸ”„ å»é‡: ç§»é™¤ {removed} æ¡é‡å¤æ–°é—»")
        
        self.results = unique_results
    
    def save_results(self):
        """ä¿å­˜ç»“æœ"""
        if not self.results:
            print("\nâš ï¸  æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ–°é—»")
            return
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        script_dir = os.path.dirname(os.path.abspath(__file__))
        output_dir = os.path.join(script_dir, '../data/raw')
        os.makedirs(output_dir, exist_ok=True)
        
        # ä¿å­˜ä¸º JSON
        date_str = datetime.now().strftime('%Y%m%d')
        output_file = os.path.join(output_dir, f'{self.sector}_newspaper_{date_str}.json')
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ å·²ä¿å­˜åˆ°: {output_file}")
        print(f"ğŸ“Š å…±ä¿å­˜: {len(self.results)} æ¡æ–°é—»")

if __name__ == '__main__':
    import argparse
    
    parser = argparse.ArgumentParser(description='Newspaper4k æ–°é—»æå–å™¨')
    parser.add_argument('--sector', required=True, choices=['healthcare', 'education'], 
                        help='æ¿å—: healthcare æˆ– education')
    parser.add_argument('--hours', type=int, default=24, help='æ—¶é—´èŒƒå›´ï¼ˆå°æ—¶ï¼‰')
    
    args = parser.parse_args()
    
    crawler = Newspaper4kCrawler(args.sector, args.hours)
    crawler.crawl_all_sources()
    crawler.save_results()



