#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é«˜çº§æ–°é—»çˆ¬è™« V2 - ä½¿ç”¨requests-htmlæ”¯æŒJavaScriptæ¸²æŸ“
"""

import json
import os
from datetime import datetime
from requests_html import HTMLSession
import time

class AdvancedNewsCrawlerV2:
    """é«˜çº§æ–°é—»çˆ¬è™« - ä½¿ç”¨requests-html"""
    
    def __init__(self, sector):
        self.sector = sector
        self.results = []
        self.config = self.load_config()
        self.keywords = self.config['sectors'][sector]['keywords']
        self.session = HTMLSession()
    
    def load_config(self):
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        script_dir = os.path.dirname(os.path.abspath(__file__))
        config_path = os.path.join(script_dir, '../references/config.json')
        with open(config_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    
    def match_keywords(self, title):
        """æ£€æŸ¥æ ‡é¢˜æ˜¯å¦åŒ…å«å…³é”®è¯"""
        for keyword in self.keywords:
            if keyword in title:
                return True
        return False
    
    def crawl_mohrss(self):
        """çˆ¬å–äººç¤¾éƒ¨åœ°æ–¹åŠ¨æ€"""
        url = "https://www.mohrss.gov.cn/SYrlzyhshbzb/dongtaixinwen/dfdt/"
        print(f"\nğŸ” çˆ¬å–: äººç¤¾éƒ¨åœ°æ–¹åŠ¨æ€")
        
        try:
            r = self.session.get(url, timeout=15)
            r.html.render(timeout=20)
            
            links = r.html.find('ul.list_16 li a')
            
            count = 0
            for link in links[:30]:
                try:
                    title = link.text.strip()
                    href = link.absolute_links
                    
                    if title and len(title) > 10 and self.match_keywords(title):
                        href_url = list(href)[0] if href else url
                        self.results.append({
                            'title': title,
                            'url': href_url,
                            'source': 'äººç¤¾éƒ¨',
                            'date': datetime.now().strftime('%Y-%m-%d')
                        })
                        count += 1
                except:
                    continue
            
            print(f"  âœ“ æ‰¾åˆ° {count} æ¡åŒ¹é…æ–°é—»")
        except Exception as e:
            print(f"  âœ— çˆ¬å–å¤±è´¥: {e}")
    
    def crawl_cs_com_cn(self):
        """çˆ¬å–ä¸­å›½è¯åˆ¸æŠ¥è´¢ç»è¦é—»"""
        url = "https://www.cs.com.cn/xwzx/hg/"
        print(f"\nğŸ” çˆ¬å–: ä¸­å›½è¯åˆ¸æŠ¥è´¢ç»è¦é—»")
        
        try:
            r = self.session.get(url, timeout=15)
            r.html.render(timeout=20)
            
            links = r.html.find('div.news-list li a, ul.news-list li a, div.list a')
            
            count = 0
            for link in links[:30]:
                try:
                    title = link.text.strip()
                    href = link.absolute_links
                    
                    if title and len(title) > 10 and self.match_keywords(title):
                        href_url = list(href)[0] if href else url
                        self.results.append({
                            'title': title,
                            'url': href_url,
                            'source': 'ä¸­å›½è¯åˆ¸æŠ¥',
                            'date': datetime.now().strftime('%Y-%m-%d')
                        })
                        count += 1
                except:
                    continue
            
            print(f"  âœ“ æ‰¾åˆ° {count} æ¡åŒ¹é…æ–°é—»")
        except Exception as e:
            print(f"  âœ— çˆ¬å–å¤±è´¥: {e}")
    
    def crawl_cls_cn(self):
        """çˆ¬å–è´¢è”ç¤¾å¤´æ¡"""
        url = "https://www.cls.cn/depth?id=1000"
        print(f"\nğŸ” çˆ¬å–: è´¢è”ç¤¾å¤´æ¡")
        
        try:
            r = self.session.get(url, timeout=15)
            r.html.render(timeout=20)
            
            links = r.html.find('div.depth-item a.item-title, div.article-item a')
            
            count = 0
            for link in links[:30]:
                try:
                    title = link.text.strip()
                    href = link.absolute_links
                    
                    if title and len(title) > 10 and self.match_keywords(title):
                        href_url = list(href)[0] if href else url
                        self.results.append({
                            'title': title,
                            'url': href_url,
                            'source': 'è´¢è”ç¤¾',
                            'date': datetime.now().strftime('%Y-%m-%d')
                        })
                        count += 1
                except:
                    continue
            
            print(f"  âœ“ æ‰¾åˆ° {count} æ¡åŒ¹é…æ–°é—»")
        except Exception as e:
            print(f"  âœ— çˆ¬å–å¤±è´¥: {e}")
    
    def crawl_guandian_cn(self):
        """çˆ¬å–è§‚ç‚¹ç½‘èµ„è®¯"""
        url = "https://www.guandian.cn/news/"
        print(f"\nğŸ” çˆ¬å–: è§‚ç‚¹ç½‘èµ„è®¯")
        
        try:
            r = self.session.get(url, timeout=15)
            r.html.render(timeout=20)
            
            links = r.html.find('div.news-item a, li.news-item a, div.article a')
            
            count = 0
            for link in links[:30]:
                try:
                    title = link.text.strip()
                    href = link.absolute_links
                    
                    if title and len(title) > 10 and self.match_keywords(title):
                        href_url = list(href)[0] if href else url
                        self.results.append({
                            'title': title,
                            'url': href_url,
                            'source': 'è§‚ç‚¹ç½‘',
                            'date': datetime.now().strftime('%Y-%m-%d')
                        })
                        count += 1
                except:
                    continue
            
            print(f"  âœ“ æ‰¾åˆ° {count} æ¡åŒ¹é…æ–°é—»")
        except Exception as e:
            print(f"  âœ— çˆ¬å–å¤±è´¥: {e}")
    
    def crawl_jjckb(self):
        """çˆ¬å–ç»æµå‚è€ƒæŠ¥è¦é—»"""
        url = "http://jjckb.xinhuanet.com/yw.htm"
        print(f"\nğŸ” çˆ¬å–: ç»æµå‚è€ƒæŠ¥è¦é—»")
        
        try:
            r = self.session.get(url, timeout=15)
            links = r.html.find('ul.news-list li a, div.news-list a, div.list a')
            
            count = 0
            for link in links[:30]:
                try:
                    title = link.text.strip()
                    href = link.absolute_links
                    
                    if title and len(title) > 10 and self.match_keywords(title):
                        href_url = list(href)[0] if href else url
                        self.results.append({
                            'title': title,
                            'url': href_url,
                            'source': 'ç»æµå‚è€ƒæŠ¥',
                            'date': datetime.now().strftime('%Y-%m-%d')
                        })
                        count += 1
                except:
                    continue
            
            print(f"  âœ“ æ‰¾åˆ° {count} æ¡åŒ¹é…æ–°é—»")
        except Exception as e:
            print(f"  âœ— çˆ¬å–å¤±è´¥: {e}")
    
    def fetch_article_content(self, url):
        """çˆ¬å–æ–‡ç« å†…å®¹"""
        try:
            r = self.session.get(url, timeout=15)
            content_selectors = [
                'div.article-content', 'div.content', 'div.news-content',
                'div.detail-content', 'article', 'div#content'
            ]
            
            for selector in content_selectors:
                elements = r.html.find(selector)
                if elements:
                    content = elements[0].text.strip()
                    if content and len(content) > 100:
                        return content
            return ""
        except:
            return ""
    
    def crawl_all_sources(self, fetch_content=False):
        """æ‰¹é‡çˆ¬å–æ‰€æœ‰é…ç½®çš„æ–°é—»æº"""
        print(f"\n{'='*60}")
        print(f"å¼€å§‹çˆ¬å– {self.config['sectors'][self.sector]['name']} æ¿å—")
        print(f"{'='*60}")
        
        if self.sector == 'education':
            self.crawl_mohrss()
        elif self.sector == 'healthcare':
            self.crawl_cs_com_cn()
            self.crawl_cls_cn()
            self.crawl_guandian_cn()
            self.crawl_jjckb()
        
        if fetch_content and self.results:
            print(f"\nğŸ“„ å¼€å§‹çˆ¬å–æ–‡ç« å†…å®¹...")
            for i, news in enumerate(self.results, 1):
                print(f"  {i}/{len(self.results)}: {news['title'][:30]}...")
                content = self.fetch_article_content(news['url'])
                news['content'] = content
                time.sleep(1)
    
    def save_results(self):
        """ä¿å­˜ç»“æœ"""
        if not self.results:
            print("\nâš ï¸  æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ–°é—»")
            return
        
        script_dir = os.path.dirname(os.path.abspath(__file__))
        data_dir = os.path.join(script_dir, '../data/raw')
        os.makedirs(data_dir, exist_ok=True)
        
        date_str = datetime.now().strftime('%Y%m%d')
        filename = os.path.join(data_dir, f"{self.sector}_advanced_{date_str}.json")
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ å·²ä¿å­˜åˆ°: {filename}")
        print(f"ğŸ“Š å…±ä¿å­˜: {len(self.results)} æ¡æ–°é—»")


def main():
    import argparse
    parser = argparse.ArgumentParser(description='é«˜çº§æ–°é—»çˆ¬è™«ï¼ˆæ”¯æŒJSæ¸²æŸ“ï¼‰')
    parser.add_argument('--sector', required=True, 
                       choices=['education', 'healthcare'],
                       help='æ¿å—')
    parser.add_argument('--content', action='store_true',
                       help='æ˜¯å¦çˆ¬å–æ–‡ç« å†…å®¹')
    args = parser.parse_args()
    
    crawler = AdvancedNewsCrawlerV2(args.sector)
    crawler.crawl_all_sources(fetch_content=args.content)
    crawler.save_results()


if __name__ == '__main__':
    main()
