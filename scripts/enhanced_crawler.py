#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ–°é—»çˆ¬è™«ç³»ç»Ÿ - å¢å¼ºç‰ˆ
æ”¯æŒå¤šç§æ–°é—»æºçš„çˆ¬å–
"""

import json
import os
import sys
from datetime import datetime
import requests
from bs4 import BeautifulSoup
import time
import re

class EnhancedNewsCrawler:
    """å¢å¼ºç‰ˆæ–°é—»çˆ¬è™«"""
    
    def __init__(self, sector, count=10):
        self.sector = sector
        self.count = count
        self.config = self.load_config()
        self.results = []
        self.session = requests.Session()
        
        # è®¾ç½®è¯·æ±‚å¤´
        self.session.headers.update({
            'User-Agent': self.config['crawler_settings']['user_agent'],
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Connection': 'keep-alive',
        })
    
    def load_config(self):
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        script_dir = os.path.dirname(os.path.abspath(__file__))
        config_path = os.path.join(script_dir, '../references/config.json')
        with open(config_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    
    def crawl(self):
        """ä¸»çˆ¬å–æ–¹æ³•"""
        print(f"\n{'='*60}")
        print(f"ğŸ•·ï¸  æ–°é—»çˆ¬è™« - {self.config['sectors'][self.sector]['name']}")
        print(f"ğŸ“Š ç›®æ ‡æ•°é‡: {self.count} æ¡")
        print(f"{'='*60}\n")
        
        # çˆ¬å–é…ç½®çš„æ–°é—»æº
        news_sources = self.config['sectors'][self.sector].get('news_sources', [])
        
        for source in news_sources:
            print(f"ğŸ“ {source['name']}")
            self.crawl_source(source)
            time.sleep(2)  # ç¤¼è²Œå»¶è¿Ÿ
        
        # å»é‡
        self.deduplicate()
        
        print(f"\nâœ… çˆ¬å–å®Œæˆï¼å…±è·å– {len(self.results)} æ¡æ–°é—»")
        return self.results
    
    def crawl_source(self, source):
        """æ ¹æ®ä¸åŒçš„ç½‘ç«™ç±»å‹é€‰æ‹©çˆ¬å–æ–¹æ³•"""
        try:
            # å¥åº·æŠ¥è¡Œä¸šå¿«è®¯ - ç‰¹æ®Šå¤„ç†ï¼ˆJSONæå–ï¼‰
            if source['name'] == 'å¥åº·æŠ¥è¡Œä¸šå¿«è®¯':
                self.crawl_jkb_industry_news()
            # åŒ»è¯ç½‘æœ€æ–°èµ„è®¯
            elif source['name'] == 'åŒ»è¯ç½‘æœ€æ–°èµ„è®¯':
                self.crawl_pharmnet_news(source['url'])
            # å›½å®¶åŒ»ç–—ä¿éšœå±€
            elif source['name'] == 'å›½å®¶åŒ»ç–—ä¿éšœå±€':
                self.crawl_nhsa(source)
            # å›½å®¶å«ç”Ÿå¥åº·å§”å‘˜ä¼š
            elif source['name'] == 'å›½å®¶å«ç”Ÿå¥åº·å§”å‘˜ä¼š':
                self.crawl_nhc(source)
            # å…¶ä»–é€šç”¨ç½‘ç«™
            else:
                self.crawl_generic_site(source)
                
        except Exception as e:
            print(f"  âœ— çˆ¬å–å¤±è´¥: {e}")
    
    def crawl_jkb_industry_news(self):
        """çˆ¬å–å¥åº·æŠ¥è¡Œä¸šå¿«è®¯ï¼ˆJSONæå–æ–¹å¼ï¼‰"""
        try:
            url = 'https://www.jkb.com.cn/news/industryNews'
            response = self.session.get(url, timeout=10)
            response.encoding = 'utf-8'
            
            soup = BeautifulSoup(response.text, 'html.parser')
            data_div = soup.find('div', id='ttde_data')
            
            if data_div:
                json_text = data_div.get_text().strip()
                news_list = json.loads(json_text)
                
                count = 0
                for news in news_list[:5]:  # é™åˆ¶5æ¡
                    news_data = {
                        'title': news['title'],
                        'url': f"https://www.jkb.com.cn/news/industryNews/{news['url']}",
                        'source': 'å¥åº·æŠ¥',
                        'date': datetime.now().strftime('%Y-%m-%d'),
                        'summary': news['description'][:200]
                    }
                    self.results.append(news_data)
                    count += 1
                
                print(f"  âœ“ è·å– {count} æ¡æ–°é—»")
            else:
                print(f"  âœ— æœªæ‰¾åˆ°æ•°æ®")
                
        except Exception as e:
            print(f"  âœ— çˆ¬å–å¤±è´¥: {e}")
    
    def crawl_pharmnet_news(self, url):
        """çˆ¬å–åŒ»è¯ç½‘æœ€æ–°èµ„è®¯"""
        try:
            response = self.session.get(url, timeout=10)
            response.encoding = 'gbk'  # åŒ»è¯ç½‘ä½¿ç”¨gbkç¼–ç 
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # æŸ¥æ‰¾æ–°é—»åˆ—è¡¨
            news_links = soup.find_all('a', href=re.compile(r'/news/\d+/\d+/\d+/\d+\.html'))
            
            count = 0
            for link in news_links[:5]:  # é™åˆ¶5æ¡
                title = link.get_text().strip()
                href = link.get('href', '')
                
                if not title or len(title) < 10:
                    continue
                
                if href.startswith('/'):
                    href = 'https://news.pharmnet.com.cn' + href
                
                news_data = {
                    'title': title,
                    'url': href,
                    'source': 'åŒ»è¯ç½‘',
                    'date': datetime.now().strftime('%Y-%m-%d'),
                    'summary': ''
                }
                
                self.results.append(news_data)
                count += 1
            
            print(f"  âœ“ è·å– {count} æ¡æ–°é—»")
            
        except Exception as e:
            print(f"  âœ— çˆ¬å–å¤±è´¥: {e}")
    
    def crawl_generic_site(self, source):
        """é€šç”¨ç½‘ç«™çˆ¬å–æ–¹æ³•"""
        try:
            response = self.session.get(source['url'], timeout=10)
            response.encoding = 'utf-8'
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # é€šç”¨çš„æ–°é—»é“¾æ¥æŸ¥æ‰¾
            news_links = soup.find_all('a', href=True)
            
            count = 0
            for link in news_links:
                title = link.get_text().strip()
                href = link.get('href', '')
                
                # è¿‡æ»¤æ¡ä»¶
                if not title or len(title) < 10 or len(title) > 100:
                    continue
                
                # è¡¥å…¨URL
                if href.startswith('/'):
                    from urllib.parse import urlparse
                    parsed = urlparse(source['url'])
                    href = f"{parsed.scheme}://{parsed.netloc}{href}"
                elif not href.startswith('http'):
                    continue
                
                news_data = {
                    'title': title,
                    'url': href,
                    'source': source['name'],
                    'date': datetime.now().strftime('%Y-%m-%d'),
                    'summary': ''
                }
                
                self.results.append(news_data)
                count += 1
                
                if count >= 3:  # æ¯ä¸ªé€šç”¨ç½‘ç«™é™åˆ¶3æ¡
                    break
            
            print(f"  âœ“ è·å– {count} æ¡æ–°é—»")
            
        except Exception as e:
            print(f"  âœ— çˆ¬å–å¤±è´¥: {e}")
    
    def crawl_nhsa(self, source):
        """çˆ¬å–å›½å®¶åŒ»ç–—ä¿éšœå±€"""
        try:
            urls = source.get('urls', [])
            limit = source.get('limit', 3)
            total_count = 0
            
            for url in urls:
                response = self.session.get(url, timeout=10)
                response.encoding = source.get('encoding', 'utf-8')
                
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # æŸ¥æ‰¾æ–°é—»åˆ—è¡¨ - ä½¿ç”¨æ­£ç¡®çš„é€‰æ‹©å™¨
                news_list = soup.find('ul', class_='infoList')
                if not news_list:
                    continue
                
                news_links = news_list.find_all('a', href=True)
                
                count = 0
                for link in news_links:
                    if count >= limit:
                        break
                    
                    title = link.get_text().strip()
                    href = link.get('href', '')
                    
                    # è¿‡æ»¤ï¼šåªè¦æœ‰å®é™…æ ‡é¢˜çš„é“¾æ¥
                    if not title or len(title) < 10:
                        continue
                    
                    # è¡¥å…¨URL
                    if href.startswith('./'):
                        href = url.rsplit('/', 1)[0] + '/' + href[2:]
                    elif href.startswith('/'):
                        href = 'https://www.nhsa.gov.cn' + href
                    elif not href.startswith('http'):
                        href = 'https://www.nhsa.gov.cn' + href
                    
                    news_data = {
                        'title': title,
                        'url': href,
                        'source': 'å›½å®¶åŒ»ä¿å±€',
                        'date': datetime.now().strftime('%Y-%m-%d'),
                        'summary': ''
                    }
                    
                    self.results.append(news_data)
                    count += 1
                    total_count += 1
            
            print(f"  âœ“ è·å– {total_count} æ¡æ–°é—»")
            
        except Exception as e:
            print(f"  âœ— çˆ¬å–å¤±è´¥: {e}")
    
    def crawl_nhc(self, source):
        """çˆ¬å–å›½å®¶å«ç”Ÿå¥åº·å§”å‘˜ä¼š"""
        try:
            # å›½å®¶å«å¥å§”æœ‰åçˆ¬è™«ï¼Œéœ€è¦ç‰¹æ®Šå¤„ç†
            print(f"  âš ï¸  å›½å®¶å«å¥å§”æœ‰åçˆ¬è™«æœºåˆ¶ï¼Œæš‚æ—¶è·³è¿‡")
            print(f"  ğŸ’¡ å»ºè®®ï¼šä½¿ç”¨æµè§ˆå™¨è‡ªåŠ¨åŒ–æˆ–RSSè®¢é˜…")
            
        except Exception as e:
            print(f"  âœ— çˆ¬å–å¤±è´¥: {e}")
    
    def deduplicate(self):
        """å»é‡åŠŸèƒ½"""
        seen_titles = set()
        unique_results = []
        
        for news in self.results:
            title = news['title']
            if title not in seen_titles:
                seen_titles.add(title)
                unique_results.append(news)
        
        removed = len(self.results) - len(unique_results)
        if removed > 0:
            print(f"\nğŸ”„ å»é‡: ç§»é™¤ {removed} æ¡é‡å¤æ–°é—»")
        
        self.results = unique_results
    
    def save_results(self):
        """ä¿å­˜ç»“æœ"""
        script_dir = os.path.dirname(os.path.abspath(__file__))
        data_dir = os.path.join(script_dir, '../data/raw')
        os.makedirs(data_dir, exist_ok=True)
        
        date_str = datetime.now().strftime('%Y%m%d')
        filename = os.path.join(data_dir, f"{self.sector}_{date_str}.json")
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ å·²ä¿å­˜åˆ°: {filename}")
        print(f"ğŸ“Š å…±ä¿å­˜: {len(self.results)} æ¡æ–°é—»")


def main():
    import argparse
    parser = argparse.ArgumentParser(description='å¢å¼ºç‰ˆæ–°é—»çˆ¬è™«')
    parser.add_argument('--sector', required=True, 
                       choices=['education', 'healthcare'],
                       help='æ¿å—: education æˆ– healthcare')
    parser.add_argument('--count', type=int, default=10,
                       help='ç›®æ ‡æ•°é‡')
    args = parser.parse_args()
    
    crawler = EnhancedNewsCrawler(args.sector, args.count)
    crawler.crawl()
    crawler.save_results()


if __name__ == '__main__':
    main()
