#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ–°é—»çˆ¬è™«ç³»ç»Ÿ - å®Œæ•´ç‰ˆ
æ”¯æŒä¸‰ç§çˆ¬å–æ–¹å¼ï¼š
1. å…³é”®è¯æœç´¢ï¼ˆé€šè¿‡ Clawdbot web_searchï¼‰
2. ä¸“ä¸šç½‘ç«™çˆ¬å–ï¼ˆåŒ»è¯ç½‘ã€å¥åº·æŠ¥ï¼‰
3. RSSè®¢é˜…ï¼ˆå¤‡ç”¨ï¼‰
"""

import argparse
import json
import os
import sys
from datetime import datetime
import requests
from bs4 import BeautifulSoup
import time

class NewsCrawler:
    """æ–°é—»çˆ¬è™«ä¸»ç±»"""
    
    def __init__(self, sector, count=10):
        self.sector = sector
        self.count = count
        self.config = self.load_config()
        self.results = []
        
    def load_config(self):
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        config_path = os.path.join(os.path.dirname(__file__), '../references/config.json')
        with open(config_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    
    def crawl(self):
        """ä¸»çˆ¬å–æ–¹æ³• - åè°ƒæ‰€æœ‰çˆ¬å–æ–¹å¼"""
        print(f"\n{'='*60}")
        print(f"å¼€å§‹çˆ¬å– {self.config['sectors'][self.sector]['name']} æ¿å—æ–°é—»")
        print(f"ç›®æ ‡æ•°é‡: {self.count} æ¡")
        print(f"{'='*60}\n")
        
        # æ–¹å¼1: å…³é”®è¯æœç´¢ï¼ˆä¸»è¦æ–¹å¼ï¼‰
        print("ğŸ“¡ æ–¹å¼1: å…³é”®è¯æœç´¢...")
        self.crawl_by_keywords()
        
        # æ–¹å¼2: ä¸“ä¸šç½‘ç«™çˆ¬å–ï¼ˆåŒ»ç–—å¥åº·æ¿å—ï¼‰
        if self.sector == 'healthcare':
            print("\nğŸ¥ æ–¹å¼2: ä¸“ä¸šç½‘ç«™çˆ¬å–...")
            self.crawl_professional_sites()
        
        # å»é‡
        self.deduplicate()
        
        print(f"\nâœ“ çˆ¬å–å®Œæˆï¼å…±è·å– {len(self.results)} æ¡æ–°é—»")
        return self.results
    
    def crawl_by_keywords(self):
        """
        æ–¹å¼1: å…³é”®è¯æœç´¢çˆ¬å–
        
        åŸç†è®²è§£ï¼š
        - ä½¿ç”¨æœç´¢å¼•æ“APIæœç´¢å…³é”®è¯
        - è¿‡æ»¤æŒ‡å®šçš„å¯ä¿¡æ–°é—»æº
        - æå–æ ‡é¢˜ã€é“¾æ¥ã€æ‘˜è¦ç­‰ä¿¡æ¯
        """
        keywords = self.config['sectors'][self.sector]['keywords']
        trusted_sources = self.config['sectors'][self.sector]['trusted_sources']
        
        # é™åˆ¶å…³é”®è¯æ•°é‡ï¼Œé¿å…è¿‡åº¦è¯·æ±‚
        for keyword in keywords[:4]:
            print(f"  æœç´¢å…³é”®è¯: {keyword}")
            
            # è¿™é‡Œéœ€è¦è°ƒç”¨ Clawdbot çš„ web_search
            # ç”±äºåœ¨è„šæœ¬ä¸­æ— æ³•ç›´æ¥è°ƒç”¨ï¼Œæˆ‘ä»¬æä¾›ä¸€ä¸ªå ä½ç¬¦
            # å®é™…ä½¿ç”¨æ—¶ï¼Œåº”è¯¥é€šè¿‡ Clawdbot è°ƒç”¨
            
            # æ¨¡æ‹Ÿæœç´¢ç»“æœï¼ˆå®é™…åº”è¯¥è°ƒç”¨ web_searchï¼‰
            print(f"    âš ï¸  éœ€è¦é€šè¿‡ Clawdbot web_search å·¥å…·æœç´¢")
            print(f"    æç¤º: åœ¨ Clawdbot ä¸­è¿è¡Œæ­¤è„šæœ¬ï¼Œæˆ–ä½¿ç”¨ API æ–¹å¼")
            
            time.sleep(1)  # é¿å…è¯·æ±‚è¿‡å¿«
    
    def crawl_professional_sites(self):
        """
        æ–¹å¼2: ä¸“ä¸šç½‘ç«™çˆ¬å–
        
        åŸç†è®²è§£ï¼š
        1. å‘é€HTTPè¯·æ±‚è·å–ç½‘é¡µHTML
        2. ä½¿ç”¨BeautifulSoupè§£æHTML
        3. æ ¹æ®ç½‘é¡µç»“æ„æå–æ–°é—»åˆ—è¡¨
        4. æå–æ¯æ¡æ–°é—»çš„æ ‡é¢˜ã€é“¾æ¥ã€æ—¶é—´ç­‰
        """
        if 'ä¸“ä¸šç½‘ç«™' not in self.config['sectors'][self.sector]:
            return
        
        sites = self.config['sectors'][self.sector]['ä¸“ä¸šç½‘ç«™']
        
        for site in sites:
            print(f"  çˆ¬å–ç½‘ç«™: {site['name']}")
            try:
                # è¿™é‡Œæ˜¯ä¸“ä¸šç½‘ç«™çˆ¬å–çš„ç¤ºä¾‹
                # å®é™…éœ€è¦æ ¹æ®æ¯ä¸ªç½‘ç«™çš„å…·ä½“ç»“æ„è°ƒæ•´
                self.crawl_site(site)
            except Exception as e:
                print(f"    âœ— çˆ¬å–å¤±è´¥: {e}")
    
    def crawl_site(self, site):
        """
        çˆ¬å–å•ä¸ªç½‘ç«™
        
        çˆ¬è™«ä¸‰æ­¥éª¤è¯¦è§£ï¼š
        """
        # ç¬¬1æ­¥ï¼šå‘é€HTTPè¯·æ±‚
        headers = {
            'User-Agent': self.config['crawler_settings']['user_agent']
        }
        
        response = requests.get(
            site['url'], 
            headers=headers,
            timeout=self.config['crawler_settings']['timeout']
        )
        
        if response.status_code != 200:
            print(f"    âœ— è¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
            return
        
        # ç¬¬2æ­¥ï¼šè§£æHTML
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # ç¬¬3æ­¥ï¼šæå–æ–°é—»åˆ—è¡¨
        # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦æ ¹æ®å…·ä½“ç½‘ç«™çš„HTMLç»“æ„è°ƒæ•´
        # è¿™æ˜¯ä¸€ä¸ªé€šç”¨ç¤ºä¾‹
        news_items = soup.find_all('a', class_='news-title')  # ç¤ºä¾‹é€‰æ‹©å™¨
        
        count = 0
        for item in news_items[:5]:  # é™åˆ¶æ¯ä¸ªç½‘ç«™5æ¡
            try:
                title = item.get_text().strip()
                url = item.get('href', '')
                
                # è¡¥å…¨ç›¸å¯¹é“¾æ¥
                if url.startswith('/'):
                    url = site['url'].rstrip('/') + url
                
                news_data = {
                    'title': title,
                    'url': url,
                    'source': site['name'],
                    'date': datetime.now().strftime('%Y-%m-%d'),
                    'content': ''  # éœ€è¦è¿›ä¸€æ­¥çˆ¬å–è¯¦æƒ…é¡µ
                }
                
                self.results.append(news_data)
                count += 1
                
            except Exception as e:
                continue
        
        print(f"    âœ“ è·å– {count} æ¡æ–°é—»")
    
    def deduplicate(self):
        """
        å»é‡åŠŸèƒ½
        
        åŸç†ï¼šæ ¹æ®æ ‡é¢˜æˆ–URLå»é™¤é‡å¤çš„æ–°é—»
        """
        seen_titles = set()
        unique_results = []
        
        for news in self.results:
            title = news['title']
            if title not in seen_titles:
                seen_titles.add(title)
                unique_results.append(news)
        
        removed = len(self.results) - len(unique_results)
        if removed > 0:
            print(f"\nğŸ”„ å»é‡: ç§»é™¤ {removed} æ¡é‡å¤æ–°é—»")
        
        self.results = unique_results
    
    def save_results(self):
        """ä¿å­˜çˆ¬å–ç»“æœåˆ°JSONæ–‡ä»¶"""
        os.makedirs('../data/raw', exist_ok=True)
        date_str = datetime.now().strftime('%Y%m%d')
        filename = f"../data/raw/{self.sector}_{date_str}.json"
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ å·²ä¿å­˜åˆ°: {filename}")
        print(f"ğŸ“Š å…±ä¿å­˜: {len(self.results)} æ¡æ–°é—»")


def main():
    parser = argparse.ArgumentParser(description='æ–°é—»çˆ¬è™«ç³»ç»Ÿ')
    parser.add_argument('--sector', required=True, 
                       choices=['education', 'healthcare'],
                       help='æ¿å—: education(æ•™è‚²äººæ‰) æˆ– healthcare(åŒ»ç–—å¥åº·)')
    parser.add_argument('--count', type=int, default=10,
                       help='ç›®æ ‡æ•°é‡ (é»˜è®¤: 10)')
    args = parser.parse_args()
    
    crawler = NewsCrawler(args.sector, args.count)
    crawler.crawl()
    crawler.save_results()


if __name__ == '__main__':
    main()
