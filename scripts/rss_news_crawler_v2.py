#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
RSS æ–°é—»èšåˆå™¨ V2 - ä» RSS æºè·å–æ–°é—»ï¼ˆæ”¹è¿›ç‰ˆï¼šæ·»åŠ æ—¶é—´è¿‡æ»¤ï¼‰
"""

import requests
from bs4 import BeautifulSoup
import json
import time
from datetime import datetime, timedelta
from dateutil import parser as date_parser
import os

class RSSNewsCrawler:
    """RSS æ–°é—»èšåˆå™¨"""
    
    def __init__(self, sector, hours=24):
        self.sector = sector
        self.hours = hours  # æ—¶é—´èŒƒå›´ï¼ˆå°æ—¶ï¼‰
        self.results = []
        self.session = requests.Session()
        
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
        })
        
        # RSS æ–°é—»æºåˆ—è¡¨ï¼ˆæ‰©å……ç‰ˆï¼‰
        self.rss_sources = {
            'healthcare': [
                {
                    'name': 'äººæ°‘ç½‘å¥åº·',
                    'url': 'http://health.people.com.cn/rss/health.xml'
                },
                {
                    'name': 'æ–°åç½‘å¥åº·',
                    'url': 'http://www.xinhuanet.com/health/news_health.xml'
                },
                {
                    'name': 'å¤®è§†ç½‘å¥åº·',
                    'url': 'http://health.cctv.com/rss/health.xml'
                },
                {
                    'name': 'å¥åº·æŠ¥',
                    'url': 'http://www.jkb.com.cn/rss.xml'
                },
                {
                    'name': 'ä¸é¦™å›­',
                    'url': 'https://www.dxy.cn/feed'
                }
            ],
            'education': [
                {
                    'name': 'äººæ°‘ç½‘æ•™è‚²',
                    'url': 'http://edu.people.com.cn/rss/edu.xml'
                },
                {
                    'name': 'æ–°åç½‘æ•™è‚²',
                    'url': 'http://www.xinhuanet.com/edu/news_edu.xml'
                },
                {
                    'name': 'ä¸­å›½æ•™è‚²æ–°é—»ç½‘',
                    'url': 'http://www.jyb.cn/rss/jyb.xml'
                },
                {
                    'name': 'å¤®è§†ç½‘æ•™è‚²',
                    'url': 'http://edu.cctv.com/rss/edu.xml'
                }
            ]
        }
    
    def parse_pubdate(self, pubdate_str):
        """
        è§£æå‘å¸ƒæ—¶é—´
        
        å‚æ•°:
            pubdate_str: æ—¶é—´å­—ç¬¦ä¸²
        
        è¿”å›:
            datetime å¯¹è±¡ï¼Œè§£æå¤±è´¥è¿”å› None
        """
        if not pubdate_str:
            return None
        
        try:
            # ä½¿ç”¨ dateutil.parser è‡ªåŠ¨è§£æå¤šç§æ—¶é—´æ ¼å¼
            return date_parser.parse(pubdate_str)
        except:
            return None
    
    def is_recent_news(self, pubdate_str):
        """
        åˆ¤æ–­æ–°é—»æ˜¯å¦åœ¨æŒ‡å®šæ—¶é—´èŒƒå›´å†…
        
        å‚æ•°:
            pubdate_str: æ—¶é—´å­—ç¬¦ä¸²
        
        è¿”å›:
            True è¡¨ç¤ºåœ¨æ—¶é—´èŒƒå›´å†…ï¼ŒFalse è¡¨ç¤ºä¸åœ¨
        """
        pub_datetime = self.parse_pubdate(pubdate_str)
        
        if not pub_datetime:
            # å¦‚æœæ— æ³•è§£ææ—¶é—´ï¼Œä¿å®ˆèµ·è§è¿”å› False
            return False
        
        # è®¡ç®—æ—¶é—´å·®
        now = datetime.now(pub_datetime.tzinfo) if pub_datetime.tzinfo else datetime.now()
        time_diff = now - pub_datetime
        
        # åˆ¤æ–­æ˜¯å¦åœ¨æŒ‡å®šæ—¶é—´èŒƒå›´å†…
        return time_diff <= timedelta(hours=self.hours)
    
    def fetch_rss(self, rss_url, source_name):
        """
        è·å– RSS æºï¼ˆå¸¦æ—¶é—´è¿‡æ»¤ï¼‰
        
        å‚æ•°:
            rss_url: RSS æº URL
            source_name: æ¥æºåç§°
        """
        print(f"\nğŸ” è·å– RSS: {source_name}")
        
        try:
            response = self.session.get(rss_url, timeout=10)
            response.encoding = 'utf-8'
            
            soup = BeautifulSoup(response.text, 'xml')
            items = soup.find_all('item')
            
            news_items = []
            filtered_count = 0
            
            for item in items:
                title_tag = item.find('title')
                link_tag = item.find('link')
                pubdate_tag = item.find('pubDate')
                
                if title_tag and link_tag:
                    title = title_tag.get_text().strip()
                    url = link_tag.get_text().strip()
                    pubdate = pubdate_tag.get_text().strip() if pubdate_tag else ''
                    
                    # æ—¶é—´è¿‡æ»¤ï¼šåªä¿ç•™æœ€è¿‘çš„æ–°é—»
                    if pubdate and self.is_recent_news(pubdate):
                        news_items.append({
                            'title': title,
                            'url': url,
                            'source': source_name,
                            'pubdate': pubdate,
                            'date': datetime.now().strftime('%Y-%m-%d')
                        })
                    else:
                        filtered_count += 1
            
            print(f"  âœ“ æ‰¾åˆ° {len(news_items)} æ¡æœ€è¿‘æ–°é—»ï¼ˆè¿‡æ»¤æ‰ {filtered_count} æ¡æ—§æ–°é—»ï¼‰")
            return news_items
            
        except Exception as e:
            print(f"  âœ— è·å–å¤±è´¥: {e}")
            return []
    
    def crawl_all_sources(self):
        """çˆ¬å–æ‰€æœ‰ RSS æº"""
        print(f"\n{'='*60}")
        print(f"å¼€å§‹çˆ¬å– RSS æ–°é—»æºï¼ˆæœ€è¿‘ {self.hours} å°æ—¶ï¼‰")
        print(f"{'='*60}")
        
        sources = self.rss_sources.get(self.sector, [])
        
        for source in sources:
            news_items = self.fetch_rss(source['url'], source['name'])
            self.results.extend(news_items)
            time.sleep(1)
        
        # å»é‡
        self.deduplicate()
    
    def deduplicate(self):
        """å»é‡"""
        seen_titles = set()
        unique_results = []
        
        for news in self.results:
            title = news['title']
            if title not in seen_titles:
                seen_titles.add(title)
                unique_results.append(news)
        
        removed = len(self.results) - len(unique_results)
        if removed > 0:
            print(f"\nğŸ”„ å»é‡: ç§»é™¤ {removed} æ¡é‡å¤æ–°é—»")
        
        self.results = unique_results
    
    def save_results(self):
        """ä¿å­˜ç»“æœ"""
        if not self.results:
            print("\nâš ï¸  æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ–°é—»")
            return
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        script_dir = os.path.dirname(os.path.abspath(__file__))
        output_dir = os.path.join(script_dir, '../data/raw')
        os.makedirs(output_dir, exist_ok=True)
        
        # ä¿å­˜ä¸º JSON
        date_str = datetime.now().strftime('%Y%m%d')
        output_file = os.path.join(output_dir, f'{self.sector}_rss_{date_str}.json')
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ å·²ä¿å­˜åˆ°: {output_file}")
        print(f"ğŸ“Š å…±ä¿å­˜: {len(self.results)} æ¡æ–°é—»")
        
        # æ‰“å°å‰5æ¡æ ‡é¢˜
        print(f"\nğŸ“° å‰5æ¡æ–°é—»æ ‡é¢˜ï¼š")
        for i, news in enumerate(self.results[:5], 1):
            print(f"{i}. {news['title']}")
            print(f"   å‘å¸ƒæ—¶é—´: {news['pubdate']}")

if __name__ == '__main__':
    import argparse
    
    parser = argparse.ArgumentParser(description='RSS æ–°é—»èšåˆå™¨ V2ï¼ˆå¸¦æ—¶é—´è¿‡æ»¤ï¼‰')
    parser.add_argument('--sector', required=True, choices=['healthcare', 'education'], 
                        help='æ¿å—: healthcare æˆ– education')
    parser.add_argument('--hours', type=int, default=24,
                        help='æ—¶é—´èŒƒå›´ï¼ˆå°æ—¶ï¼‰ï¼Œé»˜è®¤24å°æ—¶')
    
    args = parser.parse_args()
    
    crawler = RSSNewsCrawler(args.sector, args.hours)
    crawler.crawl_all_sources()
    crawler.save_results()

