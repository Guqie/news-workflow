#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é«˜çº§æ–°é—»çˆ¬è™« V3 - ä½¿ç”¨playwrightæ”¯æŒJavaScriptæ¸²æŸ“
"""

import json
import os
from datetime import datetime
from playwright.sync_api import sync_playwright
import time

class AdvancedNewsCrawlerV3:
    """é«˜çº§æ–°é—»çˆ¬è™« - ä½¿ç”¨playwright"""
    
    def __init__(self, sector):
        self.sector = sector
        self.results = []
        self.config = self.load_config()
        self.keywords = self.config['sectors'][sector]['keywords']
        self.playwright = None
        self.browser = None
        self.page = None
    
    def load_config(self):
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        script_dir = os.path.dirname(os.path.abspath(__file__))
        config_path = os.path.join(script_dir, '../references/config.json')
        with open(config_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    
    def start_browser(self):
        """å¯åŠ¨æµè§ˆå™¨"""
        self.playwright = sync_playwright().start()
        self.browser = self.playwright.chromium.launch(headless=True)
        self.page = self.browser.new_page()
    
    def close_browser(self):
        """å…³é—­æµè§ˆå™¨"""
        if self.page:
            self.page.close()
        if self.browser:
            self.browser.close()
        if self.playwright:
            self.playwright.stop()
    
    def match_keywords(self, title):
        """æ£€æŸ¥æ ‡é¢˜æ˜¯å¦åŒ…å«å…³é”®è¯"""
        for keyword in self.keywords:
            if keyword in title:
                return True
        return False
    
    def crawl_mohrss(self):
        """çˆ¬å–äººç¤¾éƒ¨åœ°æ–¹åŠ¨æ€"""
        url = "https://www.mohrss.gov.cn/SYrlzyhshbzb/dongtaixinwen/dfdt/"
        print(f"\nğŸ” çˆ¬å–: äººç¤¾éƒ¨åœ°æ–¹åŠ¨æ€")
        
        try:
            self.page.goto(url, timeout=30000)
            self.page.wait_for_timeout(3000)
            
            links = self.page.query_selector_all('ul.list_16 li a')
            
            count = 0
            for link in links[:30]:
                try:
                    title = link.inner_text().strip()
                    href = link.get_attribute('href')
                    
                    if title and len(title) > 10 and self.match_keywords(title):
                        if href and not href.startswith('http'):
                            href = 'https://www.mohrss.gov.cn' + href
                        
                        self.results.append({
                            'title': title,
                            'url': href,
                            'source': 'äººç¤¾éƒ¨',
                            'date': datetime.now().strftime('%Y-%m-%d')
                        })
                        count += 1
                except:
                    continue
            
            print(f"  âœ“ æ‰¾åˆ° {count} æ¡åŒ¹é…æ–°é—»")
        except Exception as e:
            print(f"  âœ— çˆ¬å–å¤±è´¥: {e}")
    
    def crawl_cs_com_cn(self):
        """çˆ¬å–ä¸­å›½è¯åˆ¸æŠ¥è´¢ç»è¦é—»"""
        url = "https://www.cs.com.cn/xwzx/hg/"
        print(f"\nğŸ” çˆ¬å–: ä¸­å›½è¯åˆ¸æŠ¥è´¢ç»è¦é—»")
        
        try:
            self.page.goto(url, timeout=30000)
            self.page.wait_for_timeout(3000)
            
            # ä¿®æ”¹é€‰æ‹©å™¨ï¼šç›´æ¥é€‰æ‹©å¸¦titleå±æ€§çš„aæ ‡ç­¾
            links = self.page.query_selector_all('li a[title]')
            
            count = 0
            for link in links[:30]:
                try:
                    title = link.get_attribute('title') or link.inner_text().strip()
                    href = link.get_attribute('href')
                    
                    if title and len(title) > 10 and self.match_keywords(title):
                        if href and not href.startswith('http'):
                            href = 'https://www.cs.com.cn' + href
                        
                        self.results.append({
                            'title': title,
                            'url': href,
                            'source': 'ä¸­å›½è¯åˆ¸æŠ¥',
                            'date': datetime.now().strftime('%Y-%m-%d')
                        })
                        count += 1
                except:
                    continue
            
            print(f"  âœ“ æ‰¾åˆ° {count} æ¡åŒ¹é…æ–°é—»")
        except Exception as e:
            print(f"  âœ— çˆ¬å–å¤±è´¥: {e}")
    
    def crawl_cls_cn(self):
        """çˆ¬å–è´¢è”ç¤¾å¤´æ¡"""
        url = "https://www.cls.cn/depth?id=1000"
        print(f"\nğŸ” çˆ¬å–: è´¢è”ç¤¾å¤´æ¡")
        
        try:
            self.page.goto(url, timeout=30000)
            self.page.wait_for_timeout(3000)
            
            links = self.page.query_selector_all('div.depth-item a.item-title, div.article-item a')
            
            count = 0
            for link in links[:30]:
                try:
                    title = link.inner_text().strip()
                    href = link.get_attribute('href')
                    
                    if title and len(title) > 10 and self.match_keywords(title):
                        if href and not href.startswith('http'):
                            href = 'https://www.cls.cn' + href
                        
                        self.results.append({
                            'title': title,
                            'url': href,
                            'source': 'è´¢è”ç¤¾',
                            'date': datetime.now().strftime('%Y-%m-%d')
                        })
                        count += 1
                except:
                    continue
            
            print(f"  âœ“ æ‰¾åˆ° {count} æ¡åŒ¹é…æ–°é—»")
        except Exception as e:
            print(f"  âœ— çˆ¬å–å¤±è´¥: {e}")
    
    def crawl_guandian_cn(self):
        """çˆ¬å–è§‚ç‚¹ç½‘èµ„è®¯"""
        url = "https://www.guandian.cn/news/"
        print(f"\nğŸ” çˆ¬å–: è§‚ç‚¹ç½‘èµ„è®¯")
        
        try:
            self.page.goto(url, timeout=30000)
            self.page.wait_for_timeout(3000)
            
            links = self.page.query_selector_all('div.news-item a, li.news-item a, div.article a')
            
            count = 0
            for link in links[:30]:
                try:
                    title = link.inner_text().strip()
                    href = link.get_attribute('href')
                    
                    if title and len(title) > 10 and self.match_keywords(title):
                        if href and not href.startswith('http'):
                            href = 'https://www.guandian.cn' + href
                        
                        self.results.append({
                            'title': title,
                            'url': href,
                            'source': 'è§‚ç‚¹ç½‘',
                            'date': datetime.now().strftime('%Y-%m-%d')
                        })
                        count += 1
                except:
                    continue
            
            print(f"  âœ“ æ‰¾åˆ° {count} æ¡åŒ¹é…æ–°é—»")
        except Exception as e:
            print(f"  âœ— çˆ¬å–å¤±è´¥: {e}")
    
    def crawl_jjckb(self):
        """çˆ¬å–ç»æµå‚è€ƒæŠ¥è¦é—»"""
        url = "http://jjckb.xinhuanet.com/yw.htm"
        print(f"\nğŸ” çˆ¬å–: ç»æµå‚è€ƒæŠ¥è¦é—»")
        
        try:
            self.page.goto(url, timeout=30000)
            self.page.wait_for_timeout(2000)
            
            links = self.page.query_selector_all('ul.news-list li a, div.news-list a, div.list a')
            
            count = 0
            for link in links[:30]:
                try:
                    title = link.inner_text().strip()
                    href = link.get_attribute('href')
                    
                    if title and len(title) > 10 and self.match_keywords(title):
                        self.results.append({
                            'title': title,
                            'url': href,
                            'source': 'ç»æµå‚è€ƒæŠ¥',
                            'date': datetime.now().strftime('%Y-%m-%d')
                        })
                        count += 1
                except:
                    continue
            
            print(f"  âœ“ æ‰¾åˆ° {count} æ¡åŒ¹é…æ–°é—»")
        except Exception as e:
            print(f"  âœ— çˆ¬å–å¤±è´¥: {e}")
    
    def fetch_article_content(self, url):
        """çˆ¬å–æ–‡ç« å†…å®¹"""
        try:
            self.page.goto(url, timeout=30000)
            self.page.wait_for_timeout(2000)
            
            content_selectors = [
                'div.article-content', 'div.content', 'div.news-content',
                'div.detail-content', 'article', 'div#content'
            ]
            
            for selector in content_selectors:
                try:
                    element = self.page.query_selector(selector)
                    if element:
                        content = element.inner_text().strip()
                        if content and len(content) > 100:
                            return content
                except:
                    continue
            
            return ""
        except:
            return ""
    
    def crawl_all_sources(self, fetch_content=False):
        """æ‰¹é‡çˆ¬å–æ‰€æœ‰é…ç½®çš„æ–°é—»æº"""
        print(f"\n{'='*60}")
        print(f"å¼€å§‹çˆ¬å– {self.config['sectors'][self.sector]['name']} æ¿å—")
        print(f"{'='*60}")
        
        self.start_browser()
        
        try:
            if self.sector == 'education':
                self.crawl_mohrss()
            elif self.sector == 'healthcare':
                self.crawl_cs_com_cn()
                self.crawl_cls_cn()
                self.crawl_guandian_cn()
                self.crawl_jjckb()
            
            if fetch_content and self.results:
                print(f"\nğŸ“„ å¼€å§‹çˆ¬å–æ–‡ç« å†…å®¹...")
                for i, news in enumerate(self.results, 1):
                    print(f"  {i}/{len(self.results)}: {news['title'][:30]}...")
                    content = self.fetch_article_content(news['url'])
                    news['content'] = content
                    time.sleep(1)
        finally:
            self.close_browser()
    
    def save_results(self):
        """ä¿å­˜ç»“æœ"""
        if not self.results:
            print("\nâš ï¸  æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ–°é—»")
            return
        
        script_dir = os.path.dirname(os.path.abspath(__file__))
        data_dir = os.path.join(script_dir, '../data/raw')
        os.makedirs(data_dir, exist_ok=True)
        
        date_str = datetime.now().strftime('%Y%m%d')
        filename = os.path.join(data_dir, f"{self.sector}_advanced_{date_str}.json")
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ å·²ä¿å­˜åˆ°: {filename}")
        print(f"ğŸ“Š å…±ä¿å­˜: {len(self.results)} æ¡æ–°é—»")


def main():
    import argparse
    parser = argparse.ArgumentParser(description='é«˜çº§æ–°é—»çˆ¬è™«V3ï¼ˆplaywrightï¼‰')
    parser.add_argument('--sector', required=True, 
                       choices=['education', 'healthcare'],
                       help='æ¿å—')
    parser.add_argument('--content', action='store_true',
                       help='æ˜¯å¦çˆ¬å–æ–‡ç« å†…å®¹')
    args = parser.parse_args()
    
    crawler = AdvancedNewsCrawlerV3(args.sector)
    crawler.crawl_all_sources(fetch_content=args.content)
    crawler.save_results()


if __name__ == '__main__':
    main()
