#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç»Ÿä¸€æ–°é—»èšåˆå™¨ - æ•´åˆæ‰€æœ‰æ–°é—»æº
"""

import json
import os
import subprocess
from datetime import datetime
from collections import defaultdict

class NewsAggregator:
    """ç»Ÿä¸€æ–°é—»èšåˆå™¨"""
    
    def __init__(self, sector, hours=24):
        self.sector = sector
        self.hours = hours
        self.all_news = []
        self.script_dir = os.path.dirname(os.path.abspath(__file__))
        self.data_dir = os.path.join(self.script_dir, '../data/raw')
        
        # åŠ è½½é…ç½®
        config_path = os.path.join(self.script_dir, '../references/config.json')
        with open(config_path, 'r', encoding='utf-8') as f:
            self.config = json.load(f)
    
    def run_google_news_crawler(self):
        """è¿è¡Œ Google æ–°é—»çˆ¬è™«"""
        print("\n" + "="*60)
        print("1. è¿è¡Œ Google æ–°é—»çˆ¬è™«")
        print("="*60)
        
        if self.sector == 'healthcare':
            keywords = ["åŒ»è¯äº§ä¸š", "ç”Ÿç‰©åŒ»è¯", "åŒ»ç–—å¥åº·", "åŒ»ä¿æ”¹é©"]
        else:
            keywords = ["äººæ‰æ”¿ç­–", "æ•™è‚²æ”¹é©", "äººæ‰åŸ¹å…»", "èŒä¸šæ•™è‚²"]
        
        cmd = [
            'python3', 'google_news_crawler.py',
            '--sector', self.sector,
            '--hours', str(self.hours),
            '--keywords'
        ] + keywords
        
        try:
            subprocess.run(cmd, cwd=self.script_dir, check=True)
            print("âœ… Google æ–°é—»çˆ¬è™«å®Œæˆ")
        except Exception as e:
            print(f"âŒ Google æ–°é—»çˆ¬è™«å¤±è´¥: {e}")
    
    def run_rss_crawler(self):
        """è¿è¡Œ RSS æ–°é—»çˆ¬è™«"""
        print("\n" + "="*60)
        print("2. è¿è¡Œ RSS æ–°é—»çˆ¬è™«")
        print("="*60)
        
        cmd = ['python3', 'rss_news_crawler.py', '--sector', self.sector]
        
        try:
            subprocess.run(cmd, cwd=self.script_dir, check=True)
            print("âœ… RSS æ–°é—»çˆ¬è™«å®Œæˆ")
        except Exception as e:
            print(f"âŒ RSS æ–°é—»çˆ¬è™«å¤±è´¥: {e}")
    
    def run_rolling_news_crawler(self):
        """è¿è¡Œæ»šåŠ¨æ–°é—»çˆ¬è™«"""
        print("\n" + "="*60)
        print("3. è¿è¡Œæ»šåŠ¨æ–°é—»çˆ¬è™«")
        print("="*60)
        
        # ä¸­å›½ç»æµç½‘å³æ—¶æ–°é—»
        cmd = [
            'python3', 'rolling_news_crawler.py',
            '--sector', self.sector,
            '--url', 'http://www.ce.cn/cysc/newmain/yc/jsxw/',
            '--pages', '3'
        ]
        
        try:
            subprocess.run(cmd, cwd=self.script_dir, check=True)
            print("âœ… æ»šåŠ¨æ–°é—»çˆ¬è™«å®Œæˆ")
        except Exception as e:
            print(f"âŒ æ»šåŠ¨æ–°é—»çˆ¬è™«å¤±è´¥: {e}")
    
    def run_newspaper_crawler(self):
        """è¿è¡Œ Newspaper4k çˆ¬è™«"""
        print("\n" + "="*60)
        print("4. è¿è¡Œ Newspaper4k æ–°é—»æå–å™¨")
        print("="*60)
        
        cmd = [
            'python3', 'newspaper_crawler.py',
            '--sector', self.sector,
            '--hours', str(self.hours)
        ]
        
        try:
            subprocess.run(cmd, cwd=self.script_dir, check=True)
            print("âœ… Newspaper4k çˆ¬è™«å®Œæˆ")
        except Exception as e:
            print(f"âŒ Newspaper4k çˆ¬è™«å¤±è´¥: {e}")
    
    def load_all_news(self):
        """åŠ è½½æ‰€æœ‰çˆ¬å–çš„æ–°é—»"""
        print("\n" + "="*60)
        print("5. æ•´åˆæ‰€æœ‰æ–°é—»æ•°æ®")
        print("="*60)
        
        date_str = datetime.now().strftime('%Y%m%d')
        
        # å®šä¹‰æ‰€æœ‰å¯èƒ½çš„æ•°æ®æ–‡ä»¶
        file_patterns = [
            f'{self.sector}_google_{date_str}.json',
            f'{self.sector}_rss_{date_str}.json',
            f'{self.sector}_rolling_{date_str}.json',
            f'{self.sector}_newspaper_{date_str}.json'
        ]
        
        for pattern in file_patterns:
            file_path = os.path.join(self.data_dir, pattern)
            if os.path.exists(file_path):
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        news_list = json.load(f)
                        self.all_news.extend(news_list)
                        print(f"  âœ“ åŠ è½½ {pattern}: {len(news_list)} æ¡")
                except Exception as e:
                    print(f"  âœ— åŠ è½½ {pattern} å¤±è´¥: {e}")
        
        print(f"\nğŸ“Š æ€»è®¡åŠ è½½: {len(self.all_news)} æ¡æ–°é—»")
    
    def deduplicate_and_sort(self):
        """å»é‡å’Œæ’åº"""
        print("\n" + "="*60)
        print("5. å»é‡å’Œæ’åº")
        print("="*60)
        
        # å»é‡
        seen_titles = set()
        unique_news = []
        
        for news in self.all_news:
            title = news.get('title', '')
            if title and title not in seen_titles:
                seen_titles.add(title)
                unique_news.append(news)
        
        removed = len(self.all_news) - len(unique_news)
        print(f"  ğŸ”„ å»é‡: ç§»é™¤ {removed} æ¡é‡å¤æ–°é—»")
        
        self.all_news = unique_news
        print(f"  ğŸ“Š å»é‡å: {len(self.all_news)} æ¡æ–°é—»")
    
    def save_aggregated_results(self):
        """ä¿å­˜èšåˆç»“æœ"""
        print("\n" + "="*60)
        print("6. ä¿å­˜èšåˆç»“æœ")
        print("="*60)
        
        if not self.all_news:
            print("âš ï¸  æ²¡æœ‰æ–°é—»æ•°æ®")
            return
        
        date_str = datetime.now().strftime('%Y%m%d')
        output_file = os.path.join(self.data_dir, f'{self.sector}_aggregated_{date_str}.json')
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(self.all_news, f, ensure_ascii=False, indent=2)
        
        print(f"  ğŸ’¾ å·²ä¿å­˜åˆ°: {output_file}")
        print(f"  ğŸ“Š å…±ä¿å­˜: {len(self.all_news)} æ¡æ–°é—»")
        
        # æ‰“å°å‰10æ¡æ ‡é¢˜
        print(f"\nğŸ“° å‰10æ¡æ–°é—»æ ‡é¢˜ï¼š")
        for i, news in enumerate(self.all_news[:10], 1):
            print(f"{i}. {news.get('title', 'æ— æ ‡é¢˜')}")
    
    def run(self):
        """è¿è¡Œå®Œæ•´æµç¨‹"""
        print("\n" + "ğŸš€"*30)
        print(f"å¼€å§‹æ–°é—»èšåˆ - {self.config['sectors'][self.sector]['name']}")
        print(f"æ—¶é—´èŒƒå›´: æœ€è¿‘ {self.hours} å°æ—¶")
        print("ğŸš€"*30)
        
        # 1. è¿è¡Œ Google æ–°é—»çˆ¬è™«
        self.run_google_news_crawler()
        
        # 2. è¿è¡Œ RSS æ–°é—»çˆ¬è™« (å·²ç¦ç”¨ - æ—¶æ•ˆæ€§å·®)
        # self.run_rss_crawler()
        
        # 3. è¿è¡Œæ»šåŠ¨æ–°é—»çˆ¬è™«
        self.run_rolling_news_crawler()
        
        # 4. è¿è¡Œ Newspaper4k çˆ¬è™«
        self.run_newspaper_crawler()
        
        # 5. åŠ è½½æ‰€æœ‰æ–°é—»
        self.load_all_news()
        
        # 6. å»é‡å’Œæ’åº
        self.deduplicate_and_sort()
        
        # 7. ä¿å­˜èšåˆç»“æœ
        self.save_aggregated_results()
        
        print("\n" + "âœ…"*30)
        print("æ–°é—»èšåˆå®Œæˆï¼")
        print("âœ…"*30)

if __name__ == '__main__':
    import argparse
    
    parser = argparse.ArgumentParser(description='ç»Ÿä¸€æ–°é—»èšåˆå™¨')
    parser.add_argument('--sector', required=True, choices=['healthcare', 'education'], 
                        help='æ¿å—: healthcare æˆ– education')
    parser.add_argument('--hours', type=int, default=24, help='æ—¶é—´èŒƒå›´ï¼ˆå°æ—¶ï¼‰')
    
    args = parser.parse_args()
    
    aggregator = NewsAggregator(args.sector, args.hours)
    aggregator.run()




