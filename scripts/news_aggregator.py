#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç»Ÿä¸€æ–°é—»èšåˆå™¨ - æ•´åˆæ‰€æœ‰æ–°é—»æº
"""

import json
import os
import subprocess
from datetime import datetime
from collections import defaultdict

class NewsAggregator:
    """ç»Ÿä¸€æ–°é—»èšåˆå™¨"""
    
    def __init__(self, sector, hours=24):
        self.sector = sector
        self.hours = hours
        self.all_news = []
        self.script_dir = os.path.dirname(os.path.abspath(__file__))
        self.data_dir = os.path.join(self.script_dir, '../data/raw')
        
        # åŠ è½½é…ç½®
        config_path = os.path.join(self.script_dir, '../references/config.json')
        with open(config_path, 'r', encoding='utf-8') as f:
            self.config = json.load(f)
    
    def run_google_news_crawler(self):
        """è¿è¡Œ Google æ–°é—»çˆ¬è™«ï¼ˆæ‰©å……ç‰ˆæ£€ç´¢ç­–ç•¥ï¼‰"""
        print("\n" + "="*60)
        print("1. è¿è¡Œ Google æ–°é—»çˆ¬è™«")
        print("="*60)
        
        if self.sector == 'healthcare':
            keywords = [
                # æ ¸å¿ƒäº§ä¸šè¯ï¼ˆ6ä¸ªï¼‰
                "åŒ»è¯äº§ä¸š å‘å±•",
                "ç”Ÿç‰©åŒ»è¯ åˆ›æ–°",
                "åŒ»ç–—å¥åº· æ”¿ç­–",
                "åŒ»ä¿ æ”¹é©",
                "ä¸­åŒ»è¯ äº§ä¸š",
                "åŒ»ç–—å™¨æ¢° åˆ›æ–°",
                # æ”¿ç­–æ”¹é©è¯ï¼ˆ5ä¸ªï¼‰
                "è¯ç›‘ æ”¹é©",
                "å¥åº·äº§ä¸š å»ºè®¾",
                "åŒ»ç–—ä¿éšœ ä½“ç³»",
                "å«ç”Ÿå¥åº· äº‹ä¸š",
                "åŒ»æ”¹ æ”¿ç­–",
                # åˆ›æ–°æŠ€æœ¯è¯ï¼ˆ5ä¸ªï¼‰
                "åŒ»å…»ç»“åˆ",
                "äº’è”ç½‘åŒ»ç–—",
                "æ™ºæ…§åŒ»ç–—",
                "åŒ»ç–—AI",
                "æ•°å­—å¥åº·",
                # åœ°åŸŸäº§ä¸šè¯ï¼ˆ4ä¸ªï¼‰
                "åŒ»è¯äº§ä¸š åŒ—äº¬",
                "ç”Ÿç‰©åŒ»è¯ ä¸Šæµ·",
                "åŒ»ç–—å¥åº· æ±Ÿè‹",
                "åŒ»è¯äº§ä¸š å¹¿ä¸œ"
            ]
        else:  # education
            keywords = [
                # æ ¸å¿ƒäººæ‰è¯ï¼ˆ6ä¸ªï¼‰
                "äººæ‰æ”¿ç­– å‘å±•",
                "æ•™è‚²æ”¹é© åˆ›æ–°",
                "äººæ‰åŸ¹å…» äº§ä¸š",
                "èŒä¸šæ•™è‚² å‘å±•",
                "é«˜æ ¡ äººæ‰",
                "æŠ€èƒ½äººæ‰ åŸ¹å…»",
                # å¼•è¿›æ”¯æŒè¯ï¼ˆ5ä¸ªï¼‰
                "ç§‘æŠ€äººæ‰ å¼•è¿›",
                "é’å¹´äººæ‰ æ”¿ç­–",
                "äººæ‰å¼•è¿› æ”¯æŒ",
                "é«˜å±‚æ¬¡äººæ‰",
                "äººæ‰æˆ˜ç•¥",
                # æ•™è‚²åˆ›æ–°è¯ï¼ˆ5ä¸ªï¼‰
                "äººå·¥æ™ºèƒ½ æ•™è‚²",
                "æ•°å­—äººæ‰ åŸ¹å…»",
                "äº§æ•™èåˆ",
                "æ ¡ä¼åˆä½œ",
                "åŒä¸€æµ å»ºè®¾",
                # åœ°åŸŸäººæ‰è¯ï¼ˆ4ä¸ªï¼‰
                "äººæ‰æ”¿ç­– åŒ—äº¬",
                "äººæ‰å¼•è¿› ä¸Šæµ·",
                "äººæ‰åŸ¹å…» æ±Ÿè‹",
                "äººæ‰æ”¿ç­– å¹¿ä¸œ"
            ]
        
        cmd = [
            'python3', 'google_news_crawler.py',
            '--sector', self.sector,
            '--hours', str(self.hours),
            '--keywords'
        ] + keywords
        
        try:
            subprocess.run(cmd, cwd=self.script_dir, check=True)
            print("âœ… Google æ–°é—»çˆ¬è™«å®Œæˆ")
        except Exception as e:
            print(f"âŒ Google æ–°é—»çˆ¬è™«å¤±è´¥: {e}")
    
    def run_rss_crawler(self):
        """è¿è¡Œ RSS æ–°é—»çˆ¬è™«"""
        print("\n" + "="*60)
        print("2. è¿è¡Œ RSS æ–°é—»çˆ¬è™«")
        print("="*60)
        
        cmd = ['python3', 'rss_news_crawler.py', '--sector', self.sector]
        
        try:
            subprocess.run(cmd, cwd=self.script_dir, check=True)
            print("âœ… RSS æ–°é—»çˆ¬è™«å®Œæˆ")
        except Exception as e:
            print(f"âŒ RSS æ–°é—»çˆ¬è™«å¤±è´¥: {e}")
    
    def run_rolling_news_crawler(self):
        """è¿è¡Œé€šç”¨æ–°é—»çˆ¬è™«ï¼ˆæ›¿ä»£æ»šåŠ¨æ–°é—»çˆ¬è™«ï¼‰"""
        print("\n" + "="*60)
        print("3. è¿è¡Œé€šç”¨æ–°é—»çˆ¬è™«")
        print("="*60)
        
        # åŒ»ç–—å¥åº·æ¿å—çš„ä¿¡æº
        if self.sector == 'healthcare':
            sources = [
                {'name': 'ä¸­å›½ç»æµç½‘', 'url': 'http://www.ce.cn/cysc/newmain/yc/jsxw/'},
                {'name': 'äººæ°‘ç½‘è´¢ç»', 'url': 'https://finance.people.com.cn/GB/70846/index.html'},
                {'name': 'ä¸­å›½è´¢ç»åŒ»è¯', 'url': 'https://finance.china.com.cn/industry/medicine/live.shtml'},
                {'name': 'ä¸­å›½ç§‘æŠ€ç½‘', 'url': 'https://www.stdaily.com/web/gdxw/node_324_2.html'}
            ]
        else:  # education
            sources = [
                {'name': 'ä¸­å›½ç»æµç½‘', 'url': 'http://www.ce.cn/cysc/newmain/yc/jsxw/'},
                {'name': 'ä¸­å›½è¥¿è—ç½‘', 'url': 'http://www.tibet.cn/cn/Instant/'},
                {'name': 'ä¸­å›½ç§‘æŠ€ç½‘', 'url': 'https://www.stdaily.com/web/gdxw/node_324_2.html'}
            ]
        
        # ä½¿ç”¨é€šç”¨çˆ¬è™«çˆ¬å–æ¯ä¸ªä¿¡æº
        for source in sources:
            print(f"\nğŸ“° çˆ¬å–: {source['name']}")
            cmd = [
                'python3', 'universal_crawler.py',
                '--sector', self.sector,
                '--url', source['url'],
                '--pages', '5'
            ]
            
            try:
                subprocess.run(cmd, cwd=self.script_dir, check=True)
            except Exception as e:
                print(f"  âš ï¸  {source['name']} çˆ¬å–å¤±è´¥: {e}")
        
        print("âœ… é€šç”¨æ–°é—»çˆ¬è™«å®Œæˆ")
    
    def run_newspaper_crawler(self):
        """è¿è¡Œ Newspaper4k çˆ¬è™«"""
        print("\n" + "="*60)
        print("4. è¿è¡Œ Newspaper4k æ–°é—»æå–å™¨")
        print("="*60)
        
        cmd = [
            'python3', 'newspaper_crawler.py',
            '--sector', self.sector,
            '--hours', str(self.hours)
        ]
        
        try:
            subprocess.run(cmd, cwd=self.script_dir, check=True)
            print("âœ… Newspaper4k çˆ¬è™«å®Œæˆ")
        except Exception as e:
            print(f"âŒ Newspaper4k çˆ¬è™«å¤±è´¥: {e}")
    
    def load_all_news(self):
        """åŠ è½½æ‰€æœ‰çˆ¬å–çš„æ–°é—»"""
        print("\n" + "="*60)
        print("5. æ•´åˆæ‰€æœ‰æ–°é—»æ•°æ®")
        print("="*60)
        
        date_str = datetime.now().strftime('%Y%m%d')
        
        # å®šä¹‰æ‰€æœ‰å¯èƒ½çš„æ•°æ®æ–‡ä»¶
        file_patterns = [
            f'{self.sector}_google_{date_str}.json',
            f'{self.sector}_rss_{date_str}.json',
            f'{self.sector}_rolling_{date_str}.json',
            f'{self.sector}_universal_{date_str}.json',  # é€šç”¨çˆ¬è™«æ•°æ®
            f'{self.sector}_newspaper_{date_str}.json'
        ]
        
        for pattern in file_patterns:
            file_path = os.path.join(self.data_dir, pattern)
            if os.path.exists(file_path):
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        news_list = json.load(f)
                        self.all_news.extend(news_list)
                        print(f"  âœ“ åŠ è½½ {pattern}: {len(news_list)} æ¡")
                except Exception as e:
                    print(f"  âœ— åŠ è½½ {pattern} å¤±è´¥: {e}")
        
        print(f"\nğŸ“Š æ€»è®¡åŠ è½½: {len(self.all_news)} æ¡æ–°é—»")
    
    def deduplicate_and_sort(self):
        """å»é‡å’Œæ’åº"""
        print("\n" + "="*60)
        print("5. å»é‡å’Œæ’åº")
        print("="*60)
        
        # å»é‡
        seen_titles = set()
        unique_news = []
        
        for news in self.all_news:
            title = news.get('title', '')
            if title and title not in seen_titles:
                seen_titles.add(title)
                unique_news.append(news)
        
        removed = len(self.all_news) - len(unique_news)
        print(f"  ğŸ”„ å»é‡: ç§»é™¤ {removed} æ¡é‡å¤æ–°é—»")
        
        self.all_news = unique_news
        print(f"  ğŸ“Š å»é‡å: {len(self.all_news)} æ¡æ–°é—»")
    
    def save_aggregated_results(self):
        """ä¿å­˜èšåˆç»“æœ"""
        print("\n" + "="*60)
        print("6. ä¿å­˜èšåˆç»“æœ")
        print("="*60)
        
        if not self.all_news:
            print("âš ï¸  æ²¡æœ‰æ–°é—»æ•°æ®")
            return
        
        date_str = datetime.now().strftime('%Y%m%d')
        output_file = os.path.join(self.data_dir, f'{self.sector}_aggregated_{date_str}.json')
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(self.all_news, f, ensure_ascii=False, indent=2)
        
        print(f"  ğŸ’¾ å·²ä¿å­˜åˆ°: {output_file}")
        print(f"  ğŸ“Š å…±ä¿å­˜: {len(self.all_news)} æ¡æ–°é—»")
        
        # æ‰“å°å‰10æ¡æ ‡é¢˜
        print(f"\nğŸ“° å‰10æ¡æ–°é—»æ ‡é¢˜ï¼š")
        for i, news in enumerate(self.all_news[:10], 1):
            print(f"{i}. {news.get('title', 'æ— æ ‡é¢˜')}")
    
    def run(self):
        """è¿è¡Œå®Œæ•´æµç¨‹"""
        print("\n" + "ğŸš€"*30)
        print(f"å¼€å§‹æ–°é—»èšåˆ - {self.config['sectors'][self.sector]['name']}")
        print(f"æ—¶é—´èŒƒå›´: æœ€è¿‘ {self.hours} å°æ—¶")
        print("ğŸš€"*30)
        
        # 1. è¿è¡Œ Google æ–°é—»çˆ¬è™«
        self.run_google_news_crawler()
        
        # 2. è¿è¡Œ RSS æ–°é—»çˆ¬è™« (å·²ç¦ç”¨ - æ—¶æ•ˆæ€§å·®)
        # self.run_rss_crawler()
        
        # 3. è¿è¡Œæ»šåŠ¨æ–°é—»çˆ¬è™«
        self.run_rolling_news_crawler()
        
        # 4. è¿è¡Œ Newspaper4k çˆ¬è™«
        self.run_newspaper_crawler()
        
        # 5. åŠ è½½æ‰€æœ‰æ–°é—»
        self.load_all_news()
        
        # 6. å»é‡å’Œæ’åº
        self.deduplicate_and_sort()
        
        # 7. ä¿å­˜èšåˆç»“æœ
        self.save_aggregated_results()
        
        print("\n" + "âœ…"*30)
        print("æ–°é—»èšåˆå®Œæˆï¼")
        print("âœ…"*30)

if __name__ == '__main__':
    import argparse
    
    parser = argparse.ArgumentParser(description='ç»Ÿä¸€æ–°é—»èšåˆå™¨')
    parser.add_argument('--sector', required=True, choices=['healthcare', 'education'], 
                        help='æ¿å—: healthcare æˆ– education')
    parser.add_argument('--hours', type=int, default=24, help='æ—¶é—´èŒƒå›´ï¼ˆå°æ—¶ï¼‰')
    
    args = parser.parse_args()
    
    aggregator = NewsAggregator(args.sector, args.hours)
    aggregator.run()




