#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é€šç”¨æ–°é—»çˆ¬è™« - è‡ªåŠ¨è¯†åˆ«ç½‘é¡µç»“æ„
åªéœ€æä¾›URLï¼Œè‡ªåŠ¨æå–æ–°é—»åˆ—è¡¨
"""

import json
import os
from datetime import datetime
import requests
from bs4 import BeautifulSoup
import time
import re
from urllib.parse import urljoin
import urllib3

# ç¦ç”¨SSLè­¦å‘Š
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

class UniversalNewsCrawler:
    """é€šç”¨æ–°é—»çˆ¬è™« - è‡ªåŠ¨è¯†åˆ«ç½‘é¡µç»“æ„"""
    
    def __init__(self, sector):
        self.sector = sector
        self.results = []
        self.session = requests.Session()
        
        # è®¾ç½®è¯·æ±‚å¤´
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
        })
        
        # åŠ è½½å…³é”®è¯
        self.keywords = self.load_keywords()
    
    def load_keywords(self):
        """åŠ è½½å…³é”®è¯"""
        script_dir = os.path.dirname(os.path.abspath(__file__))
        config_path = os.path.join(script_dir, '../references/config.json')
        
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                config = json.load(f)
                return config['sectors'][self.sector]['keywords']
        except:
            # é»˜è®¤å…³é”®è¯
            if self.sector == 'healthcare':
                return ['åŒ»ç–—', 'å¥åº·', 'åŒ»è¯', 'åŒ»é™¢', 'åŒ»ä¿']
            else:
                return ['æ•™è‚²', 'äººæ‰', 'é«˜æ ¡', 'åŸ¹è®­', 'å°±ä¸š']
    
    def auto_detect_news_list(self, soup, base_url):
        """è‡ªåŠ¨è¯†åˆ«æ–°é—»åˆ—è¡¨ - å¢å¼ºç‰ˆ"""
        news_items = []
        seen_urls = set()
        
        # ç­–ç•¥1: æŸ¥æ‰¾å¸¸è§æ–°é—»åˆ—è¡¨å®¹å™¨
        patterns = [
            r'(list|news|item|article|content)',
            r'(roll|scroll|feed)',
            r'(main|body|center)'
        ]
        
        for pattern in patterns:
            containers = soup.find_all(['ul', 'ol', 'div', 'section'], 
                                      class_=re.compile(pattern, re.I))
            
            for container in containers:
                links = container.find_all('a', href=True)
                if len(links) >= 3:  # é™ä½é˜ˆå€¼åˆ°3ä¸ª
                    for link in links[:100]:
                        title = link.get_text(strip=True)
                        url = urljoin(base_url, link['href'])
                        
                        # æå–æ—¥æœŸ
                        published = self._extract_date(link.parent)
                        
                        # è¿‡æ»¤æ¡ä»¶
                        if (title and 
                            len(title) >= 8 and len(title) <= 150 and
                            url not in seen_urls and
                            not self._is_invalid_link(url)):
                            
                            seen_urls.add(url)
                            news_items.append({
                                'title': title,
                                'url': url,
                                'source': base_url,
                                'published': published
                            })
            
            if len(news_items) >= 20:
                break
        
        # ç­–ç•¥2: æŸ¥æ‰¾å¸¦æ—¶é—´æ ‡è®°çš„é“¾æ¥
        if len(news_items) < 10:
            time_patterns = [r'\d{4}-\d{2}-\d{2}', r'\d{2}:\d{2}', r'\d{2}/\d{2}']
            all_links = soup.find_all('a', href=True)
            
            for link in all_links[:200]:
                parent = link.parent
                if parent:
                    parent_text = parent.get_text()
                    has_time = any(re.search(p, parent_text) for p in time_patterns)
                    
                    if has_time:
                        title = link.get_text(strip=True)
                        url = urljoin(base_url, link['href'])
                        published = self._extract_date(parent)
                        
                        if (title and len(title) >= 8 and 
                            url not in seen_urls and
                            not self._is_invalid_link(url)):
                            
                            seen_urls.add(url)
                            news_items.append({
                                'title': title,
                                'url': url,
                                'source': base_url,
                                'published': published
                            })
        
        return news_items
    
    def _extract_date(self, element):
        """æå–æ—¥æœŸæ—¶é—´"""
        if not element:
            return None
        
        text = element.get_text()
        
        # æ—¥æœŸæ ¼å¼
        date_patterns = [
            r'\d{4}-\d{2}-\d{2}\s+\d{2}:\d{2}',  # 2026-02-09 14:30
            r'\d{4}/\d{2}/\d{2}\s+\d{2}:\d{2}',  # 2026/02/09 14:30
            r'\d{4}å¹´\d{2}æœˆ\d{2}æ—¥',            # 2026å¹´02æœˆ09æ—¥
            r'\d{2}-\d{2}\s+\d{2}:\d{2}',        # 02-09 14:30
            r'\d{2}/\d{2}\s+\d{2}:\d{2}',        # 02/09 14:30
        ]
        
        for pattern in date_patterns:
            match = re.search(pattern, text)
            if match:
                return match.group()
        
        return None
    
    def _is_invalid_link(self, url):
        """åˆ¤æ–­æ˜¯å¦ä¸ºæ— æ•ˆé“¾æ¥"""
        invalid_patterns = [
            r'javascript:', r'#', r'mailto:', 
            r'\.(jpg|png|gif|pdf|zip|rar)$',
            r'(login|register|about|contact)'
        ]
        return any(re.search(p, url, re.I) for p in invalid_patterns)
    
    def match_keywords(self, title):
        """å…³é”®è¯åŒ¹é… - å¢å¼ºç‰ˆ"""
        # åŸºç¡€åŒ¹é…
        for keyword in self.keywords:
            if keyword in title:
                return True
        
        # æ‰©å±•åŒ¹é…ï¼šåŒä¹‰è¯å’Œç›¸å…³è¯
        if self.sector == 'healthcare':
            extended_keywords = [
                'è¯ä¼', 'è¯å‚', 'åˆ¶è¯', 'æ–°è¯', 'ä»¿åˆ¶è¯',
                'åŒ»ç”Ÿ', 'æŠ¤å£«', 'æ‚£è€…', 'ç—…äºº',
                'è¯Šæ‰€', 'å«ç”Ÿ', 'ç–¾æ§', 'CDC',
                'åŒ»å­¦', 'ä¸´åºŠ', 'æ‰‹æœ¯', 'æ²»ç–—'
            ]
        else:  # education
            extended_keywords = [
                'å­¦ç”Ÿ', 'æ•™å¸ˆ', 'è€å¸ˆ', 'æ ¡é•¿',
                'å¤§å­¦ç”Ÿ', 'ç ”ç©¶ç”Ÿ', 'åšå£«', 'ç¡•å£«',
                'æ‹›ç”Ÿ', 'è€ƒè¯•', 'å‡å­¦', 'æ¯•ä¸š',
                'ç§‘ç ”', 'å­¦æœ¯', 'è®ºæ–‡', 'è¯¾é¢˜'
            ]
        
        for keyword in extended_keywords:
            if keyword in title:
                return True
        
        return False
    
    def crawl_url(self, url, max_pages=3):
        """çˆ¬å–å•ä¸ªURLï¼ˆæ”¯æŒç¿»é¡µï¼‰"""
        print(f"\nğŸ” çˆ¬å–: {url}")
        
        for page in range(1, max_pages + 1):
            try:
                # æ„é€ ç¿»é¡µURL
                page_url = self._build_page_url(url, page)
                
                print(f"  ğŸ“„ ç¬¬ {page} é¡µ...")
                
                # ç¦ç”¨SSLéªŒè¯
                response = self.session.get(page_url, timeout=10, verify=False)
                response.encoding = response.apparent_encoding or 'utf-8'
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # è‡ªåŠ¨è¯†åˆ«æ–°é—»åˆ—è¡¨
                news_items = self.auto_detect_news_list(soup, url)
                
                if not news_items:
                    print(f"     âœ— æœªæ‰¾åˆ°æ–°é—»ï¼Œåœæ­¢ç¿»é¡µ")
                    break
                
                # å…³é”®è¯è¿‡æ»¤
                matched = 0
                for item in news_items:
                    if self.match_keywords(item['title']):
                        item['crawled_at'] = datetime.now().isoformat()
                        self.results.append(item)
                        matched += 1
                
                print(f"     âœ“ æ‰¾åˆ° {len(news_items)} æ¡æ–°é—»ï¼ŒåŒ¹é… {matched} æ¡")
                
                time.sleep(1)  # ç¤¼è²Œå»¶è¿Ÿ
                
            except Exception as e:
                print(f"     âœ— ç¬¬ {page} é¡µçˆ¬å–å¤±è´¥: {e}")
                break
    
    def _build_page_url(self, base_url, page):
        """æ„é€ ç¿»é¡µURL - å¢å¼ºç‰ˆ"""
        if page == 1:
            return base_url
        
        # å¸¸è§ç¿»é¡µæ¨¡å¼ï¼ˆæŒ‰ä¼˜å…ˆçº§æ’åºï¼‰
        patterns = [
            # æ¨¡å¼1: index.html -> index_2.html
            (r'index\.html$', f'index_{page}.html'),
            # æ¨¡å¼2: index.html -> index_2.htm
            (r'index\.htm$', f'index_{page}.htm'),
            # æ¨¡å¼3: /path/ -> /path/2/
            (r'/$', f'{page}/'),
            # æ¨¡å¼4: /path/index.html -> /path/index_2.html
            (r'/index\.html$', f'/index_{page}.html'),
            # æ¨¡å¼5: .html -> _2.html
            (r'\.html$', f'_{page}.html'),
            # æ¨¡å¼6: .htm -> _2.htm
            (r'\.htm$', f'_{page}.htm'),
            # æ¨¡å¼7: /node_324_2.html -> /node_324_3.html
            (r'_(\d+)\.html$', f'_{page}.html'),
        ]
        
        for pattern, replacement in patterns:
            if re.search(pattern, base_url):
                return re.sub(pattern, replacement, base_url)
        
        # é»˜è®¤ï¼šæ·»åŠ pageå‚æ•°
        separator = '&' if '?' in base_url else '?'
        return f"{base_url}{separator}page={page}"
    
    def save_results(self):
        """ä¿å­˜ç»“æœï¼ˆè¿½åŠ æ¨¡å¼ + æ™ºèƒ½å»é‡ï¼‰"""
        if not self.results:
            print("\nâš ï¸  æ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„æ–°é—»")
            return
        
        script_dir = os.path.dirname(os.path.abspath(__file__))
        data_dir = os.path.join(script_dir, '../data/raw')
        os.makedirs(data_dir, exist_ok=True)
        
        date_str = datetime.now().strftime('%Y%m%d')
        filename = os.path.join(data_dir, f"{self.sector}_universal_{date_str}.json")
        
        # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œå…ˆåŠ è½½ç°æœ‰æ•°æ®
        existing_data = []
        if os.path.exists(filename):
            try:
                with open(filename, 'r', encoding='utf-8') as f:
                    existing_data = json.load(f)
            except:
                existing_data = []
        
        # åˆå¹¶æ•°æ®
        all_data = existing_data + self.results
        
        # æ™ºèƒ½å»é‡ï¼šURL + æ ‡é¢˜ç›¸ä¼¼åº¦
        unique_data = []
        seen_urls = set()
        seen_titles = set()
        
        for item in all_data:
            url = item.get('url', '')
            title = item.get('title', '')
            
            # URLå»é‡
            if url in seen_urls:
                continue
            
            # æ ‡é¢˜ç›¸ä¼¼åº¦å»é‡ï¼ˆç®€åŒ–ç‰ˆï¼‰
            title_key = self._normalize_title(title)
            if title_key in seen_titles:
                continue
            
            seen_urls.add(url)
            seen_titles.add(title_key)
            unique_data.append(item)
        
        # ä¿å­˜
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(unique_data, f, ensure_ascii=False, indent=2)
        
        removed = len(all_data) - len(unique_data)
        print(f"\nğŸ’¾ å·²ä¿å­˜åˆ°: {filename}")
        print(f"ğŸ“Š å…±ä¿å­˜: {len(unique_data)} æ¡æ–°é—»ï¼ˆæœ¬æ¬¡æ–°å¢: {len(self.results)} æ¡ï¼Œå»é‡: {removed} æ¡ï¼‰")
    
    def _normalize_title(self, title):
        """æ ‡é¢˜å½’ä¸€åŒ–ï¼ˆç”¨äºå»é‡ï¼‰"""
        # ç§»é™¤ç©ºæ ¼ã€æ ‡ç‚¹
        normalized = re.sub(r'[\s\-_â€”\|ï½œ]', '', title)
        # åªä¿ç•™å‰30ä¸ªå­—ç¬¦
        return normalized[:30]

if __name__ == '__main__':
    import argparse
    
    parser = argparse.ArgumentParser(description='é€šç”¨æ–°é—»çˆ¬è™«')
    parser.add_argument('--sector', required=True, choices=['healthcare', 'education'],
                        help='æ¿å—: healthcare æˆ– education')
    parser.add_argument('--url', required=True, help='æ–°é—»ç½‘ç«™URL')
    parser.add_argument('--pages', type=int, default=3, help='ç¿»é¡µæ•°ï¼ˆæš‚ä¸æ”¯æŒï¼‰')
    
    args = parser.parse_args()
    
    crawler = UniversalNewsCrawler(args.sector)
    crawler.crawl_url(args.url, args.pages)
    crawler.save_results()

