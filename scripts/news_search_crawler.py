#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ–°é—»æœç´¢çˆ¬è™« - ä»æ–°é—»èšåˆç½‘ç«™æœç´¢æ–°é—»
"""

import requests
from bs4 import BeautifulSoup
import json
import time
from datetime import datetime
import os

class NewsSearchCrawler:
    """æ–°é—»æœç´¢çˆ¬è™«"""
    
    def __init__(self, sector):
        self.sector = sector
        self.config = self.load_config()
        self.results = []
        self.session = requests.Session()
        
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
        })
    
    def load_config(self):
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        script_dir = os.path.dirname(os.path.abspath(__file__))
        config_path = os.path.join(script_dir, '../references/config.json')
        with open(config_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    
    def search_baidu_news(self, keyword, max_results=10):
        """
        æœç´¢ç™¾åº¦æ–°é—»
        
        å‚æ•°:
            keyword: æœç´¢å…³é”®è¯
            max_results: æœ€å¤§ç»“æœæ•°
        """
        print(f"\nğŸ” æœç´¢ç™¾åº¦æ–°é—»: {keyword}")
        
        # ç™¾åº¦æ–°é—»æœç´¢ URL
        search_url = f"https://www.baidu.com/s?tn=news&word={keyword}"
        
        try:
            response = self.session.get(search_url, timeout=10)
            response.encoding = 'utf-8'
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # æŸ¥æ‰¾æ–°é—»ç»“æœ
            news_items = []
            results = soup.find_all('div', class_='result')
            
            for result in results[:max_results]:
                # æå–æ ‡é¢˜
                title_tag = result.find('h3')
                if not title_tag:
                    continue
                
                title = title_tag.get_text().strip()
                
                # æå–é“¾æ¥
                link_tag = title_tag.find('a')
                url = link_tag.get('href', '') if link_tag else ''
                
                # æå–æ¥æºå’Œæ—¶é—´
                source_tag = result.find('span', class_='c-color-gray2')
                source = source_tag.get_text().strip() if source_tag else 'æœªçŸ¥æ¥æº'
                
                news_items.append({
                    'title': title,
                    'url': url,
                    'source': source,
                    'keyword': keyword,
                    'date': datetime.now().strftime('%Y-%m-%d')
                })
            
            print(f"  âœ“ æ‰¾åˆ° {len(news_items)} æ¡æ–°é—»")
            return news_items
            
        except Exception as e:
            print(f"  âœ— æœç´¢å¤±è´¥: {e}")
            return []
    
    def search_sogou_news(self, keyword, max_results=10):
        """
        æœç´¢æœç‹—æ–°é—»
        
        å‚æ•°:
            keyword: æœç´¢å…³é”®è¯
            max_results: æœ€å¤§ç»“æœæ•°
        """
        print(f"\nğŸ” æœç´¢æœç‹—æ–°é—»: {keyword}")
        
        # æœç‹—æ–°é—»æœç´¢ URL
        search_url = f"https://news.sogou.com/news?query={keyword}"
        
        try:
            response = self.session.get(search_url, timeout=10)
            response.encoding = 'utf-8'
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # æŸ¥æ‰¾æ–°é—»ç»“æœ
            news_items = []
            results = soup.find_all('div', class_='news-box')
            
            for result in results[:max_results]:
                # æå–æ ‡é¢˜
                title_tag = result.find('h3')
                if not title_tag:
                    continue
                
                title = title_tag.get_text().strip()
                
                # æå–é“¾æ¥
                link_tag = title_tag.find('a')
                url = link_tag.get('href', '') if link_tag else ''
                
                # æå–æ¥æº
                source_tag = result.find('span', class_='news-from')
                source = source_tag.get_text().strip() if source_tag else 'æœªçŸ¥æ¥æº'
                
                news_items.append({
                    'title': title,
                    'url': url,
                    'source': source,
                    'keyword': keyword,
                    'date': datetime.now().strftime('%Y-%m-%d')
                })
            
            print(f"  âœ“ æ‰¾åˆ° {len(news_items)} æ¡æ–°é—»")
            return news_items
            
        except Exception as e:
            print(f"  âœ— æœç´¢å¤±è´¥: {e}")
            return []
    
    def search_with_keywords(self, keywords, max_results_per_keyword=10):
        """
        ä½¿ç”¨å¤šä¸ªå…³é”®è¯æœç´¢
        
        å‚æ•°:
            keywords: å…³é”®è¯åˆ—è¡¨
            max_results_per_keyword: æ¯ä¸ªå…³é”®è¯çš„æœ€å¤§ç»“æœæ•°
        """
        print(f"\n{'='*60}")
        print(f"å¼€å§‹æœç´¢ - {self.config['sectors'][self.sector]['name']}")
        print(f"{'='*60}")
        
        for keyword in keywords:
            # æœç´¢ç™¾åº¦æ–°é—»
            baidu_results = self.search_baidu_news(keyword, max_results_per_keyword)
            self.results.extend(baidu_results)
            
            time.sleep(2)  # ç¤¼è²Œå»¶è¿Ÿ
            
            # æœç´¢æœç‹—æ–°é—»
            sogou_results = self.search_sogou_news(keyword, max_results_per_keyword)
            self.results.extend(sogou_results)
            
            time.sleep(2)  # ç¤¼è²Œå»¶è¿Ÿ
        
        # å»é‡
        self.deduplicate()
    
    def deduplicate(self):
        """å»é‡åŠŸèƒ½"""
        seen_titles = set()
        unique_results = []
        
        for news in self.results:
            title = news['title']
            if title not in seen_titles:
                seen_titles.add(title)
                unique_results.append(news)
        
        removed = len(self.results) - len(unique_results)
        if removed > 0:
            print(f"\nğŸ”„ å»é‡: ç§»é™¤ {removed} æ¡é‡å¤æ–°é—»")
        
        self.results = unique_results
    
    def save_results(self):
        """ä¿å­˜ç»“æœ"""
        if not self.results:
            print("\nâš ï¸  æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ–°é—»")
            return
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        script_dir = os.path.dirname(os.path.abspath(__file__))
        output_dir = os.path.join(script_dir, '../data/raw')
        os.makedirs(output_dir, exist_ok=True)
        
        # ä¿å­˜ä¸º JSON
        date_str = datetime.now().strftime('%Y%m%d')
        output_file = os.path.join(output_dir, f'{self.sector}_search_{date_str}.json')
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ å·²ä¿å­˜åˆ°: {output_file}")
        print(f"ğŸ“Š å…±ä¿å­˜: {len(self.results)} æ¡æ–°é—»")

if __name__ == '__main__':
    import argparse
    
    parser = argparse.ArgumentParser(description='æ–°é—»æœç´¢çˆ¬è™«')
    parser.add_argument('--sector', required=True, 
                        choices=['healthcare', 'education', 'strategic_emerging', 'hightech'], 
                        help='æ¿å—: healthcare, education, strategic_emerging, hightech')
    parser.add_argument('--keywords', nargs='+', help='æœç´¢å…³é”®è¯åˆ—è¡¨')
    parser.add_argument('--count', type=int, default=10, help='æ¯ä¸ªå…³é”®è¯çš„æœ€å¤§ç»“æœæ•°')
    
    args = parser.parse_args()
    
    crawler = NewsSearchCrawler(args.sector)
    
    # å¦‚æœæ²¡æœ‰æŒ‡å®šå…³é”®è¯ï¼Œä½¿ç”¨é»˜è®¤çš„ç¬¬ä¸€ä¼˜å…ˆçº§å…³é”®è¯
    if not args.keywords:
        if args.sector == 'healthcare':
            keywords = ["åŒ»è¯äº§ä¸š å‘å±•", "ç”Ÿç‰©åŒ»è¯ åˆ›æ–°", "åŒ»ç–—å¥åº· æ”¿ç­–", "åŒ»ä¿ æ”¹é©"]
        elif args.sector == 'education':
            keywords = ["äººæ‰æ”¿ç­– å‘å±•", "æ•™è‚²æ”¹é© åˆ›æ–°", "äººæ‰åŸ¹å…» äº§ä¸š", "èŒä¸šæ•™è‚² å‘å±•"]
        elif args.sector == 'strategic_emerging':
            keywords = ["æˆ˜ç•¥æ–°å…´äº§ä¸š", "æ–°èƒ½æº å‘å±•", "æ–°ææ–™ äº§ä¸š", "æ•°å­—ç»æµ"]
        elif args.sector == 'hightech':
            keywords = ["é«˜ç§‘æŠ€äº§ä¸š", "äººå·¥æ™ºèƒ½ å‘å±•", "èŠ¯ç‰‡ äº§ä¸š", "åŠå¯¼ä½“"]
    else:
        keywords = args.keywords
    
    crawler.search_with_keywords(keywords, args.count)
    crawler.save_results()
