#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
RSS æ–°é—»èšåˆå™¨ - ä» RSS æºè·å–æ–°é—»
"""

import requests
from bs4 import BeautifulSoup
import json
import time
from datetime import datetime
import os

class RSSNewsCrawler:
    """RSS æ–°é—»èšåˆå™¨"""
    
    def __init__(self, sector):
        self.sector = sector
        self.results = []
        self.session = requests.Session()
        
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
        })
        
        # RSS æ–°é—»æºåˆ—è¡¨
        self.rss_sources = {
            'healthcare': [
                {
                    'name': 'äººæ°‘ç½‘å¥åº·',
                    'url': 'http://health.people.com.cn/rss/health.xml'
                },
                {
                    'name': 'æ–°åç½‘å¥åº·',
                    'url': 'http://www.xinhuanet.com/health/news_health.xml'
                }
            ],
            'education': [
                {
                    'name': 'äººæ°‘ç½‘æ•™è‚²',
                    'url': 'http://edu.people.com.cn/rss/edu.xml'
                },
                {
                    'name': 'æ–°åç½‘æ•™è‚²',
                    'url': 'http://www.xinhuanet.com/edu/news_edu.xml'
                }
            ]
        }
    
    def fetch_rss(self, rss_url, source_name):
        """
        è·å– RSS æº
        
        å‚æ•°:
            rss_url: RSS æº URL
            source_name: æ¥æºåç§°
        """
        print(f"\nğŸ” è·å– RSS: {source_name}")
        
        try:
            response = self.session.get(rss_url, timeout=10)
            response.encoding = 'utf-8'
            
            soup = BeautifulSoup(response.text, 'xml')
            items = soup.find_all('item')
            
            news_items = []
            for item in items:
                title_tag = item.find('title')
                link_tag = item.find('link')
                pubdate_tag = item.find('pubDate')
                
                if title_tag and link_tag:
                    title = title_tag.get_text().strip()
                    url = link_tag.get_text().strip()
                    pubdate = pubdate_tag.get_text().strip() if pubdate_tag else ''
                    
                    news_items.append({
                        'title': title,
                        'url': url,
                        'source': source_name,
                        'pubdate': pubdate,
                        'date': datetime.now().strftime('%Y-%m-%d')
                    })
            
            print(f"  âœ“ æ‰¾åˆ° {len(news_items)} æ¡æ–°é—»")
            return news_items
            
        except Exception as e:
            print(f"  âœ— è·å–å¤±è´¥: {e}")
            return []
    
    def crawl_all_sources(self):
        """çˆ¬å–æ‰€æœ‰ RSS æº"""
        print(f"\n{'='*60}")
        print(f"å¼€å§‹çˆ¬å– RSS æ–°é—»æº")
        print(f"{'='*60}")
        
        sources = self.rss_sources.get(self.sector, [])
        
        for source in sources:
            news_items = self.fetch_rss(source['url'], source['name'])
            self.results.extend(news_items)
            time.sleep(1)
        
        # å»é‡
        self.deduplicate()
    
    def deduplicate(self):
        """å»é‡"""
        seen_titles = set()
        unique_results = []
        
        for news in self.results:
            title = news['title']
            if title not in seen_titles:
                seen_titles.add(title)
                unique_results.append(news)
        
        removed = len(self.results) - len(unique_results)
        if removed > 0:
            print(f"\nğŸ”„ å»é‡: ç§»é™¤ {removed} æ¡é‡å¤æ–°é—»")
        
        self.results = unique_results
    
    def save_results(self):
        """ä¿å­˜ç»“æœ"""
        if not self.results:
            print("\nâš ï¸  æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ–°é—»")
            return
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        script_dir = os.path.dirname(os.path.abspath(__file__))
        output_dir = os.path.join(script_dir, '../data/raw')
        os.makedirs(output_dir, exist_ok=True)
        
        # ä¿å­˜ä¸º JSON
        date_str = datetime.now().strftime('%Y%m%d')
        output_file = os.path.join(output_dir, f'{self.sector}_rss_{date_str}.json')
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ å·²ä¿å­˜åˆ°: {output_file}")
        print(f"ğŸ“Š å…±ä¿å­˜: {len(self.results)} æ¡æ–°é—»")
        
        # æ‰“å°å‰5æ¡æ ‡é¢˜
        print(f"\nğŸ“° å‰5æ¡æ–°é—»æ ‡é¢˜ï¼š")
        for i, news in enumerate(self.results[:5], 1):
            print(f"{i}. {news['title']}")

if __name__ == '__main__':
    import argparse
    
    parser = argparse.ArgumentParser(description='RSS æ–°é—»èšåˆå™¨')
    parser.add_argument('--sector', required=True, choices=['healthcare', 'education'], 
                        help='æ¿å—: healthcare æˆ– education')
    
    args = parser.parse_args()
    
    crawler = RSSNewsCrawler(args.sector)
    crawler.crawl_all_sources()
    crawler.save_results()
