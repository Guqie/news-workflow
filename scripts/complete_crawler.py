#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å®Œæ•´æ–°é—»æºçˆ¬è™« - é’ˆå¯¹7ä¸ªç½‘ç«™çš„ä¸“é—¨çˆ¬å–é€»è¾‘
"""

import json
import os
from datetime import datetime
from playwright.sync_api import sync_playwright
import time

class CompleteCrawler:
    """å®Œæ•´æ–°é—»æºçˆ¬è™«"""
    
    def __init__(self, sector):
        self.sector = sector
        self.results = []
        self.config = self.load_config()
        self.keywords = self.config['sectors'][sector]['keywords']
        self.playwright = None
        self.browser = None
        self.page = None
    
    def load_config(self):
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        script_dir = os.path.dirname(os.path.abspath(__file__))
        config_path = os.path.join(script_dir, '../references/config.json')
        with open(config_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    
    def start_browser(self):
        """å¯åŠ¨æµè§ˆå™¨"""
        self.playwright = sync_playwright().start()
        self.browser = self.playwright.chromium.launch(headless=True)
        self.page = self.browser.new_page()
    
    def close_browser(self):
        """å…³é—­æµè§ˆå™¨"""
        if self.page:
            self.page.close()
        if self.browser:
            self.browser.close()
        if self.playwright:
            self.playwright.stop()
    
    def match_keywords(self, title):
        """æ£€æŸ¥æ ‡é¢˜æ˜¯å¦åŒ…å«å…³é”®è¯"""
        for keyword in self.keywords:
            if keyword in title:
                return True
        return False
    
    def crawl_workercn(self):
        """çˆ¬å–ä¸­å·¥ç½‘æ»šåŠ¨æ–°é—»"""
        url = "https://www.workercn.cn/roll/"
        print(f"\nğŸ” çˆ¬å–: ä¸­å·¥ç½‘æ»šåŠ¨æ–°é—»")
        
        try:
            self.page.goto(url, timeout=30000)
            self.page.wait_for_timeout(3000)
            
            # ä¸­å·¥ç½‘çš„æ–°é—»é“¾æ¥åœ¨ a æ ‡ç­¾ä¸­ï¼Œæ ‡é¢˜åœ¨ title å±æ€§
            links = self.page.query_selector_all('a[title]')
            
            count = 0
            for link in links[:50]:
                try:
                    title = link.get_attribute('title')
                    href = link.get_attribute('href')
                    
                    if title and len(title) > 10 and self.match_keywords(title):
                        if href and not href.startswith('http'):
                            href = 'https://www.workercn.cn' + href
                        
                        self.results.append({
                            'title': title,
                            'url': href,
                            'source': 'ä¸­å·¥ç½‘',
                            'date': datetime.now().strftime('%Y-%m-%d')
                        })
                        count += 1
                except:
                    continue
            
            print(f"  âœ“ æ‰¾åˆ° {count} æ¡åŒ¹é…æ–°é—»")
        except Exception as e:
            print(f"  âœ— çˆ¬å–å¤±è´¥: {e}")
    
    def crawl_cs_com_cn(self):
        """çˆ¬å–ä¸­å›½è¯åˆ¸æŠ¥è´¢ç»è¦é—»"""
        url = "https://www.cs.com.cn/xwzx/hg/"
        print(f"\nğŸ” çˆ¬å–: ä¸­å›½è¯åˆ¸æŠ¥è´¢ç»è¦é—»")
        
        try:
            self.page.goto(url, timeout=30000)
            self.page.wait_for_timeout(3000)
            
            # ä¸­å›½è¯åˆ¸æŠ¥çš„æ–°é—»é“¾æ¥åœ¨ li a[title] ä¸­
            links = self.page.query_selector_all('li a[title]')
            
            count = 0
            for link in links[:50]:
                try:
                    title = link.get_attribute('title')
                    href = link.get_attribute('href')
                    
                    if title and len(title) > 10 and self.match_keywords(title):
                        if href and not href.startswith('http'):
                            href = 'https://www.cs.com.cn' + href
                        
                        self.results.append({
                            'title': title,
                            'url': href,
                            'source': 'ä¸­å›½è¯åˆ¸æŠ¥',
                            'date': datetime.now().strftime('%Y-%m-%d')
                        })
                        count += 1
                except:
                    continue
            
            print(f"  âœ“ æ‰¾åˆ° {count} æ¡åŒ¹é…æ–°é—»")
        except Exception as e:
            print(f"  âœ— çˆ¬å–å¤±è´¥: {e}")
    
    def crawl_cls_cn(self):
        """çˆ¬å–è´¢è”ç¤¾å¤´æ¡"""
        url = "https://www.cls.cn/depth?id=1000"
        print(f"\nğŸ” çˆ¬å–: è´¢è”ç¤¾å¤´æ¡")
        
        try:
            self.page.goto(url, timeout=30000)
            self.page.wait_for_timeout(3000)
            
            links = self.page.query_selector_all('a')
            
            count = 0
            for link in links[:50]:
                try:
                    title = link.inner_text().strip()
                    href = link.get_attribute('href')
                    
                    if title and len(title) > 10 and self.match_keywords(title):
                        if href and not href.startswith('http'):
                            href = 'https://www.cls.cn' + href
                        
                        self.results.append({
                            'title': title,
                            'url': href,
                            'source': 'è´¢è”ç¤¾',
                            'date': datetime.now().strftime('%Y-%m-%d')
                        })
                        count += 1
                except:
                    continue
            
            print(f"  âœ“ æ‰¾åˆ° {count} æ¡åŒ¹é…æ–°é—»")
        except Exception as e:
            print(f"  âœ— çˆ¬å–å¤±è´¥: {e}")
    
    def crawl_bjd_com_cn(self):
        """çˆ¬å–äº¬æŠ¥ç½‘çƒ­ç‚¹"""
        url = "https://www.bjd.com.cn/app/rdjh/redian/"
        print(f"\nğŸ” çˆ¬å–: äº¬æŠ¥ç½‘çƒ­ç‚¹")
        
        try:
            self.page.goto(url, timeout=30000)
            self.page.wait_for_timeout(3000)
            
            links = self.page.query_selector_all('a')
            
            count = 0
            for link in links[:50]:
                try:
                    title = link.inner_text().strip()
                    href = link.get_attribute('href')
                    
                    if title and len(title) > 10 and self.match_keywords(title):
                        if href and not href.startswith('http'):
                            href = 'https://www.bjd.com.cn' + href
                        
                        self.results.append({
                            'title': title,
                            'url': href,
                            'source': 'äº¬æŠ¥ç½‘',
                            'date': datetime.now().strftime('%Y-%m-%d')
                        })
                        count += 1
                except:
                    continue
            
            print(f"  âœ“ æ‰¾åˆ° {count} æ¡åŒ¹é…æ–°é—»")
        except Exception as e:
            print(f"  âœ— çˆ¬å–å¤±è´¥: {e}")
    
    def crawl_guandian_cn(self):
        """çˆ¬å–è§‚ç‚¹ç½‘èµ„è®¯"""
        url = "https://www.guandian.cn/news/"
        print(f"\nğŸ” çˆ¬å–: è§‚ç‚¹ç½‘èµ„è®¯")
        
        try:
            self.page.goto(url, timeout=30000)
            self.page.wait_for_timeout(3000)
            
            links = self.page.query_selector_all('a')
            
            count = 0
            for link in links[:50]:
                try:
                    title = link.inner_text().strip()
                    href = link.get_attribute('href')
                    
                    if title and len(title) > 10 and self.match_keywords(title):
                        if href and not href.startswith('http'):
                            href = 'https://www.guandian.cn' + href
                        
                        self.results.append({
                            'title': title,
                            'url': href,
                            'source': 'è§‚ç‚¹ç½‘',
                            'date': datetime.now().strftime('%Y-%m-%d')
                        })
                        count += 1
                except:
                    continue
            
            print(f"  âœ“ æ‰¾åˆ° {count} æ¡åŒ¹é…æ–°é—»")
        except Exception as e:
            print(f"  âœ— çˆ¬å–å¤±è´¥: {e}")
    
    def crawl_jjckb(self):
        """çˆ¬å–ç»æµå‚è€ƒæŠ¥è¦é—»"""
        url = "http://jjckb.xinhuanet.com/yw.htm"
        print(f"\nğŸ” çˆ¬å–: ç»æµå‚è€ƒæŠ¥è¦é—»")
        
        try:
            self.page.goto(url, timeout=30000)
            self.page.wait_for_timeout(2000)
            
            links = self.page.query_selector_all('a')
            
            count = 0
            for link in links[:50]:
                try:
                    title = link.inner_text().strip()
                    href = link.get_attribute('href')
                    
                    if title and len(title) > 10 and self.match_keywords(title):
                        self.results.append({
                            'title': title,
                            'url': href,
                            'source': 'ç»æµå‚è€ƒæŠ¥',
                            'date': datetime.now().strftime('%Y-%m-%d')
                        })
                        count += 1
                except:
                    continue
            
            print(f"  âœ“ æ‰¾åˆ° {count} æ¡åŒ¹é…æ–°é—»")
        except Exception as e:
            print(f"  âœ— çˆ¬å–å¤±è´¥: {e}")
    
    def crawl_mohrss(self):
        """çˆ¬å–äººç¤¾éƒ¨åœ°æ–¹åŠ¨æ€"""
        url = "https://www.mohrss.gov.cn/SYrlzyhshbzb/dongtaixinwen/dfdt/"
        print(f"\nğŸ” çˆ¬å–: äººç¤¾éƒ¨åœ°æ–¹åŠ¨æ€")
        
        try:
            self.page.goto(url, timeout=30000)
            self.page.wait_for_timeout(3000)
            
            links = self.page.query_selector_all('a')
            
            count = 0
            for link in links[:50]:
                try:
                    title = link.inner_text().strip()
                    href = link.get_attribute('href')
                    
                    if title and len(title) > 10 and self.match_keywords(title):
                        if href and not href.startswith('http'):
                            href = 'https://www.mohrss.gov.cn' + href
                        
                        self.results.append({
                            'title': title,
                            'url': href,
                            'source': 'äººç¤¾éƒ¨',
                            'date': datetime.now().strftime('%Y-%m-%d')
                        })
                        count += 1
                except:
                    continue
            
            print(f"  âœ“ æ‰¾åˆ° {count} æ¡åŒ¹é…æ–°é—»")
        except Exception as e:
            print(f"  âœ— çˆ¬å–å¤±è´¥: {e}")
    
    def fetch_article_content(self, url):
        """çˆ¬å–æ–‡ç« å†…å®¹"""
        try:
            self.page.goto(url, timeout=30000)
            self.page.wait_for_timeout(2000)
            
            # å°è¯•å¤šç§å¸¸è§çš„æ–‡ç« å†…å®¹é€‰æ‹©å™¨
            content_selectors = [
                'div.article-content',
                'div.content',
                'div.news-content',
                'div.detail-content',
                'article',
                'div#content',
                'div.main-content',
                'div.text',
                'div.article-body'
            ]
            
            for selector in content_selectors:
                try:
                    element = self.page.query_selector(selector)
                    if element:
                        content = element.inner_text().strip()
                        if content and len(content) > 100:
                            return content
                except:
                    continue
            
            return ""
        except:
            return ""
    
    def crawl_all_sources(self, fetch_content=False):
        """æ‰¹é‡çˆ¬å–æ‰€æœ‰æ–°é—»æº"""
        print(f"\n{'='*60}")
        print(f"å¼€å§‹çˆ¬å–æ‰€æœ‰æ–°é—»æº")
        print(f"{'='*60}")
        
        self.start_browser()
        
        try:
            # çˆ¬å–æ‰€æœ‰7ä¸ªç½‘ç«™
            self.crawl_workercn()
            self.crawl_cs_com_cn()
            self.crawl_cls_cn()
            self.crawl_bjd_com_cn()
            self.crawl_guandian_cn()
            self.crawl_jjckb()
            self.crawl_mohrss()
            
            # å¦‚æœéœ€è¦çˆ¬å–æ–‡ç« å†…å®¹
            if fetch_content and self.results:
                print(f"\nğŸ“„ å¼€å§‹çˆ¬å–æ–‡ç« å†…å®¹...")
                for i, news in enumerate(self.results, 1):
                    print(f"  {i}/{len(self.results)}: {news['title'][:30]}...")
                    content = self.fetch_article_content(news['url'])
                    news['content'] = content
                    time.sleep(1)
        finally:
            self.close_browser()
    
    def save_results(self):
        """ä¿å­˜ç»“æœ"""
        if not self.results:
            print("\nâš ï¸  æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ–°é—»")
            return
        
        script_dir = os.path.dirname(os.path.abspath(__file__))
        data_dir = os.path.join(script_dir, '../data/raw')
        os.makedirs(data_dir, exist_ok=True)
        
        date_str = datetime.now().strftime('%Y%m%d')
        filename = os.path.join(data_dir, f"{self.sector}_complete_{date_str}.json")
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ å·²ä¿å­˜åˆ°: {filename}")
        print(f"ğŸ“Š å…±ä¿å­˜: {len(self.results)} æ¡æ–°é—»")
        
        # æ‰“å°å‰5æ¡æ–°é—»æ ‡é¢˜
        print(f"\nğŸ“° å‰5æ¡æ–°é—»ï¼š")
        for i, news in enumerate(self.results[:5], 1):
            print(f"{i}. {news['title']}")


def main():
    import argparse
    parser = argparse.ArgumentParser(description='å®Œæ•´æ–°é—»æºçˆ¬è™«')
    parser.add_argument('--sector', required=True, 
                       choices=['education', 'healthcare'],
                       help='æ¿å—')
    parser.add_argument('--content', action='store_true',
                       help='æ˜¯å¦çˆ¬å–æ–‡ç« å†…å®¹')
    args = parser.parse_args()
    
    crawler = CompleteCrawler(args.sector)
    crawler.crawl_all_sources(fetch_content=args.content)
    crawler.save_results()


if __name__ == '__main__':
    main()
