#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
SearXNGå…ƒæœç´¢å¼•æ“çˆ¬è™«
"""

import requests
import json
import os
from datetime import datetime
from typing import List, Dict

class SearXNGCrawler:
    """SearXNGå…ƒæœç´¢å¼•æ“çˆ¬è™«"""
    
    def __init__(self, sector, proxy=None):
        self.sector = sector
        self.config = self.load_config()
        self.results = []
        
        # SearXNGå…¬å…±å®ä¾‹åˆ—è¡¨ï¼ˆæŒ‰ä¼˜å…ˆçº§ï¼‰
        self.instances = [
            "https://searx.be",
            "https://searx.work",
            "https://search.bus-hit.me",
            "https://searx.tiekoetter.com",
            "https://searx.fmac.xyz",
            "https://search.sapti.me",
            "https://searx.prvcy.eu"
        ]
        self.current_instance = self.instances[0]
        self.timeout = 10
        
        # ä»£ç†é…ç½®
        self.proxies = None
        if proxy:
            self.proxies = {
                "http": proxy,
                "https": proxy
            }
            print(f"âœ“ ä½¿ç”¨ä»£ç†: {proxy}")
    
    def load_config(self):
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        script_dir = os.path.dirname(os.path.abspath(__file__))
        config_path = os.path.join(script_dir, '../references/config.json')
        with open(config_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    
    def search_news(self, keyword: str, time_range: str = "day") -> List[Dict]:
        """
        æœç´¢æ–°é—»
        
        å‚æ•°:
            keyword: æœç´¢å…³é”®è¯
            time_range: æ—¶é—´èŒƒå›´ (day/week/month/year)
        
        è¿”å›:
            æ–°é—»åˆ—è¡¨
        """
        print(f"\nğŸ” æœç´¢ SearXNG: {keyword}")
        
        params = {
            "q": keyword,
            "categories": "news",
            "format": "json",
            "time_range": time_range,
            "language": "zh-CN"
        }
        
        # å°è¯•å¤šä¸ªå®ä¾‹
        for instance in self.instances:
            try:
                print(f"  å°è¯•å®ä¾‹: {instance}")
                response = requests.get(
                    f"{instance}/search",
                    params=params,
                    timeout=self.timeout,
                    proxies=self.proxies
                )
                
                print(f"  çŠ¶æ€ç : {response.status_code}")
                
                if response.status_code == 200:
                    data = response.json()
                    results = data.get("results", [])
                    
                    print(f"  åŸå§‹ç»“æœæ•°: {len(results)}")
                    
                    # å¦‚æœç»“æœä¸ºç©ºï¼Œå°è¯•ä¸‹ä¸€ä¸ªå®ä¾‹
                    if not results:
                        print(f"  âš ï¸  å®ä¾‹ {instance} è¿”å›ç©ºç»“æœï¼Œå°è¯•ä¸‹ä¸€ä¸ª")
                        continue
                    
                    # è½¬æ¢ä¸ºç»Ÿä¸€æ ¼å¼
                    news_items = []
                    for item in results:
                        news_items.append({
                            "title": item.get("title", ""),
                            "url": item.get("url", ""),
                            "content": item.get("content", ""),
                            "source": item.get("engine", "æœªçŸ¥æ¥æº"),
                            "publish_date": item.get("publishedDate", ""),
                            "keyword": keyword,
                            "date": datetime.now().strftime('%Y-%m-%d')
                        })
                    
                    print(f"  âœ“ æ‰¾åˆ° {len(news_items)} æ¡æ–°é—» (å®ä¾‹: {instance})")
                    return news_items
                else:
                    print(f"  âš ï¸  å®ä¾‹ {instance} è¿”å›çŠ¶æ€ç : {response.status_code}")
                    
            except Exception as e:
                print(f"  âš ï¸  å®ä¾‹ {instance} å¤±è´¥: {e}")
                continue
        
        print(f"  âœ— æ‰€æœ‰å®ä¾‹éƒ½å¤±è´¥")
        return []
    
    def search_with_keywords(self, keywords: List[str], time_range: str = "day"):
        """ä½¿ç”¨å¤šä¸ªå…³é”®è¯æœç´¢"""
        print(f"\n{'='*60}")
        print(f"å¼€å§‹æœç´¢ SearXNG - {self.config['sectors'][self.sector]['name']}")
        print(f"æ—¶é—´èŒƒå›´: {time_range}")
        print(f"{'='*60}")
        
        for keyword in keywords:
            news_items = self.search_news(keyword, time_range)
            self.results.extend(news_items)
        
        # å»é‡
        self.deduplicate()
    
    def deduplicate(self):
        """å»é‡"""
        seen_titles = set()
        unique_results = []
        
        for news in self.results:
            title = news['title']
            if title not in seen_titles:
                seen_titles.add(title)
                unique_results.append(news)
        
        removed = len(self.results) - len(unique_results)
        if removed > 0:
            print(f"\nğŸ”„ å»é‡: ç§»é™¤ {removed} æ¡é‡å¤æ–°é—»")
        
        self.results = unique_results
    
    def save_results(self):
        """ä¿å­˜ç»“æœ"""
        if not self.results:
            print("\nâš ï¸  æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ–°é—»")
            return
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        script_dir = os.path.dirname(os.path.abspath(__file__))
        output_dir = os.path.join(script_dir, '../data/raw')
        os.makedirs(output_dir, exist_ok=True)
        
        # ä¿å­˜ä¸º JSON
        date_str = datetime.now().strftime('%Y%m%d')
        output_file = os.path.join(output_dir, f'{self.sector}_searxng_{date_str}.json')
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ å·²ä¿å­˜åˆ°: {output_file}")
        print(f"ğŸ“Š å…±ä¿å­˜: {len(self.results)} æ¡æ–°é—»")
        
        # æ‰“å°å‰5æ¡æ ‡é¢˜
        print(f"\nğŸ“° å‰5æ¡æ–°é—»æ ‡é¢˜ï¼š")
        for i, news in enumerate(self.results[:5], 1):
            print(f"{i}. {news['title']}")
            print(f"   æ¥æº: {news['source']} | å‘å¸ƒ: {news['publish_date']}")

if __name__ == '__main__':
    import argparse
    
    parser = argparse.ArgumentParser(description='SearXNGå…ƒæœç´¢å¼•æ“çˆ¬è™«')
    parser.add_argument('--sector', required=True, 
                        choices=['healthcare', 'education', 'strategic_emerging', 'hightech'], 
                        help='æ¿å—')
    parser.add_argument('--keywords', nargs='+', help='æœç´¢å…³é”®è¯åˆ—è¡¨')
    parser.add_argument('--time-range', default='day', 
                        choices=['day', 'week', 'month', 'year'],
                        help='æ—¶é—´èŒƒå›´')
    parser.add_argument('--proxy', help='ä»£ç†åœ°å€ï¼Œæ ¼å¼: http://host:port æˆ– socks5://host:port')
    
    args = parser.parse_args()
    
    crawler = SearXNGCrawler(args.sector, proxy=args.proxy)
    
    # å¦‚æœæ²¡æœ‰æŒ‡å®šå…³é”®è¯ï¼Œä½¿ç”¨é»˜è®¤å…³é”®è¯
    if not args.keywords:
        if args.sector == 'strategic_emerging':
            keywords = ["æˆ˜ç•¥æ–°å…´äº§ä¸š", "æ–°èƒ½æº", "æ–°ææ–™"]
        elif args.sector == 'hightech':
            keywords = ["é«˜ç§‘æŠ€äº§ä¸š", "äººå·¥æ™ºèƒ½", "èŠ¯ç‰‡"]
        elif args.sector == 'healthcare':
            keywords = ["åŒ»è¯äº§ä¸š", "ç”Ÿç‰©åŒ»è¯"]
        else:
            keywords = ["äººæ‰æ”¿ç­–", "æ•™è‚²æ”¹é©"]
    else:
        keywords = args.keywords
    
    crawler.search_with_keywords(keywords, args.time_range)
    crawler.save_results()
