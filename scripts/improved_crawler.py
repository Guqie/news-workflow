#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ”¹è¿›ç‰ˆæ»šåŠ¨æ–°é—»çˆ¬è™« - ä¸¤çº§å…³é”®è¯åŒ¹é…ç­–ç•¥
"""

import json
import os
from datetime import datetime
import requests
from bs4 import BeautifulSoup
import time

class ImprovedRollingNewsCrawler:
    """æ”¹è¿›ç‰ˆæ»šåŠ¨æ–°é—»çˆ¬è™« - æé«˜åŒ¹é…ç‡"""
    
    def __init__(self, sector):
        self.sector = sector
        self.config = self.load_config()
        self.results = []
        self.session = requests.Session()
        
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
        })
        
        # æ ¸å¿ƒå…³é”®è¯ï¼ˆä¸“ä¸šè¯æ±‡ï¼‰
        self.core_keywords = self.config['sectors'][sector]['keywords']
        
        # è¾…åŠ©å…³é”®è¯ï¼ˆæé«˜å¬å›ç‡ï¼‰
        self.auxiliary_keywords = ['äº§ä¸š', 'å‘å±•', 'åˆ›æ–°', 'æœåŠ¡', 'ä¿éšœ', 'æ”¹é©', 'å»ºè®¾']
    
    def load_config(self):
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        script_dir = os.path.dirname(os.path.abspath(__file__))
        config_path = os.path.join(script_dir, '../references/config.json')
        with open(config_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    
    def match_keywords(self, title):
        """
        ä¸¤çº§å…³é”®è¯åŒ¹é…ç­–ç•¥
        
        è§„åˆ™ï¼š
        1. å¿…é¡»åŒ…å«è‡³å°‘1ä¸ªæ ¸å¿ƒå…³é”®è¯
        2. æˆ–è€…åŒ…å«2ä¸ªä»¥ä¸Šè¾…åŠ©å…³é”®è¯
        """
        # æ£€æŸ¥æ ¸å¿ƒå…³é”®è¯
        core_match_count = 0
        for keyword in self.core_keywords:
            if keyword in title:
                core_match_count += 1
        
        if core_match_count >= 1:
            return True
        
        # æ£€æŸ¥è¾…åŠ©å…³é”®è¯
        aux_match_count = 0
        for keyword in self.auxiliary_keywords:
            if keyword in title:
                aux_match_count += 1
        
        if aux_match_count >= 2:
            return True
        
        return False
    
    def crawl_rolling_news(self, url, max_pages=3):
        """çˆ¬å–æ»šåŠ¨æ–°é—»"""
        print(f"\nğŸ” çˆ¬å–æ»šåŠ¨æ–°é—»: {url}")
        print(f"ğŸ“‹ åŒ¹é…ç­–ç•¥: æ ¸å¿ƒè¯({len(self.core_keywords)}ä¸ª) + è¾…åŠ©è¯({len(self.auxiliary_keywords)}ä¸ª)")
        
        for page in range(1, max_pages + 1):
            print(f"\n  ğŸ“„ ç¬¬ {page} é¡µ...")
            
            page_url = self.build_page_url(url, page)
            
            try:
                response = self.session.get(page_url, timeout=10)
                
                if 'people.com.cn' in page_url:
                    response.encoding = 'gb2312'
                else:
                    response.encoding = 'utf-8'
                
                soup = BeautifulSoup(response.text, 'html.parser')
                news_items = self.extract_news_list(soup, url)
                
                matched_count = 0
                for item in news_items:
                    if self.match_keywords(item['title']):
                        self.results.append(item)
                        matched_count += 1
                
                print(f"     âœ“ æ‰¾åˆ° {len(news_items)} æ¡æ–°é—»ï¼ŒåŒ¹é… {matched_count} æ¡")
                
                time.sleep(1)
                
            except Exception as e:
                print(f"     âœ— çˆ¬å–å¤±è´¥: {e}")
                break
        
        self.deduplicate()
    
    def build_page_url(self, base_url, page):
        """æ„é€ ç¿»é¡µURL"""
        if 'people.com.cn' in base_url:
            if page == 1:
                return base_url
            else:
                return base_url.replace('index.html', f'index{page}.html')
        elif 'sina.com.cn' in base_url:
            return base_url.replace('page=1', f'page={page}')
        else:
            separator = '&' if '?' in base_url else '?'
            return f"{base_url}{separator}page={page}"
