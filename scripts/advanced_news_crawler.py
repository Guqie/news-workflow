#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é«˜çº§æ–°é—»çˆ¬è™« - æ”¯æŒJavaScriptæ¸²æŸ“çš„ç½‘ç«™
ä½¿ç”¨Selenium + Chrome Headless
"""

import json
import os
from datetime import datetime
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import time

class AdvancedNewsCrawler:
    """é«˜çº§æ–°é—»çˆ¬è™« - æ”¯æŒJSæ¸²æŸ“"""
    
    def __init__(self, sector):
        self.sector = sector
        self.results = []
        self.config = self.load_config()
        self.keywords = self.config['sectors'][sector]['keywords']
        
        # åˆå§‹åŒ–Chromeæµè§ˆå™¨ï¼ˆæ— å¤´æ¨¡å¼ï¼‰
        chrome_options = Options()
        chrome_options.add_argument('--headless')
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-dev-shm-usage')
        chrome_options.add_argument('--disable-gpu')
        chrome_options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36')
        
        self.driver = webdriver.Chrome(options=chrome_options)
        self.driver.set_page_load_timeout(30)
    
    def load_config(self):
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        script_dir = os.path.dirname(os.path.abspath(__file__))
        config_path = os.path.join(script_dir, '../references/config.json')
        with open(config_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    
    def match_keywords(self, title):
        """æ£€æŸ¥æ ‡é¢˜æ˜¯å¦åŒ…å«å…³é”®è¯"""
        for keyword in self.keywords:
            if keyword in title:
                return True
        return False
    
    def crawl_mohrss(self):
        """çˆ¬å–äººç¤¾éƒ¨åœ°æ–¹åŠ¨æ€"""
        url = "https://www.mohrss.gov.cn/SYrlzyhshbzb/dongtaixinwen/dfdt/"
        print(f"\nğŸ” çˆ¬å–: äººç¤¾éƒ¨åœ°æ–¹åŠ¨æ€")
        
        try:
            self.driver.get(url)
            time.sleep(3)  # ç­‰å¾…é¡µé¢åŠ è½½
            
            # æŸ¥æ‰¾æ–°é—»åˆ—è¡¨
            news_items = self.driver.find_elements(By.CSS_SELECTOR, "ul.list_16 li a")
            
            for item in news_items[:30]:  # é™åˆ¶æ•°é‡
                try:
                    title = item.text.strip()
                    href = item.get_attribute('href')
                    
                    if title and len(title) > 10 and self.match_keywords(title):
                        self.results.append({
                            'title': title,
                            'url': href,
                            'source': 'äººç¤¾éƒ¨',
                            'date': datetime.now().strftime('%Y-%m-%d')
                        })
                except:
                    continue
            
            print(f"  âœ“ æ‰¾åˆ° {len([r for r in self.results if r['source']=='äººç¤¾éƒ¨'])} æ¡åŒ¹é…æ–°é—»")
        except Exception as e:
            print(f"  âœ— çˆ¬å–å¤±è´¥: {e}")
    
    def crawl_cs_com_cn(self):
        """çˆ¬å–ä¸­å›½è¯åˆ¸æŠ¥è´¢ç»è¦é—»"""
        url = "https://www.cs.com.cn/xwzx/hg/"
        print(f"\nğŸ” çˆ¬å–: ä¸­å›½è¯åˆ¸æŠ¥è´¢ç»è¦é—»")
        
        try:
            self.driver.get(url)
            time.sleep(3)
            
            # æŸ¥æ‰¾æ–°é—»åˆ—è¡¨
            news_items = self.driver.find_elements(By.CSS_SELECTOR, "div.news-list li a, ul.news-list li a")
            
            for item in news_items[:30]:
                try:
                    title = item.text.strip()
                    href = item.get_attribute('href')
                    
                    if title and len(title) > 10 and self.match_keywords(title):
                        if not href.startswith('http'):
                            href = 'https://www.cs.com.cn' + href
                        
                        self.results.append({
                            'title': title,
                            'url': href,
                            'source': 'ä¸­å›½è¯åˆ¸æŠ¥',
                            'date': datetime.now().strftime('%Y-%m-%d')
                        })
                except:
                    continue
            
            print(f"  âœ“ æ‰¾åˆ° {len([r for r in self.results if r['source']=='ä¸­å›½è¯åˆ¸æŠ¥'])} æ¡åŒ¹é…æ–°é—»")
        except Exception as e:
            print(f"  âœ— çˆ¬å–å¤±è´¥: {e}")
    
    def crawl_cls_cn(self):
        """çˆ¬å–è´¢è”ç¤¾å¤´æ¡"""
        url = "https://www.cls.cn/depth?id=1000"
        print(f"\nğŸ” çˆ¬å–: è´¢è”ç¤¾å¤´æ¡")
        
        try:
            self.driver.get(url)
            time.sleep(3)
            
            # æŸ¥æ‰¾æ–°é—»åˆ—è¡¨
            news_items = self.driver.find_elements(By.CSS_SELECTOR, "div.depth-item a.item-title")
            
            for item in news_items[:30]:
                try:
                    title = item.text.strip()
                    href = item.get_attribute('href')
                    
                    if title and len(title) > 10 and self.match_keywords(title):
                        if not href.startswith('http'):
                            href = 'https://www.cls.cn' + href
                        
                        self.results.append({
                            'title': title,
                            'url': href,
                            'source': 'è´¢è”ç¤¾',
                            'date': datetime.now().strftime('%Y-%m-%d')
                        })
                except:
                    continue
            
            print(f"  âœ“ æ‰¾åˆ° {len([r for r in self.results if r['source']=='è´¢è”ç¤¾'])} æ¡åŒ¹é…æ–°é—»")
        except Exception as e:
            print(f"  âœ— çˆ¬å–å¤±è´¥: {e}")
    
    def crawl_guandian_cn(self):
        """çˆ¬å–è§‚ç‚¹ç½‘èµ„è®¯"""
        url = "https://www.guandian.cn/news/"
        print(f"\nğŸ” çˆ¬å–: è§‚ç‚¹ç½‘èµ„è®¯")
        
        try:
            self.driver.get(url)
            time.sleep(3)
            
            news_items = self.driver.find_elements(By.CSS_SELECTOR, "div.news-item a, li.news-item a")
            
            for item in news_items[:30]:
                try:
                    title = item.text.strip()
                    href = item.get_attribute('href')
                    
                    if title and len(title) > 10 and self.match_keywords(title):
                        if not href.startswith('http'):
                            href = 'https://www.guandian.cn' + href
                        
                        self.results.append({
                            'title': title,
                            'url': href,
                            'source': 'è§‚ç‚¹ç½‘',
                            'date': datetime.now().strftime('%Y-%m-%d')
                        })
                except:
                    continue
            
            print(f"  âœ“ æ‰¾åˆ° {len([r for r in self.results if r['source']=='è§‚ç‚¹ç½‘'])} æ¡åŒ¹é…æ–°é—»")
        except Exception as e:
            print(f"  âœ— çˆ¬å–å¤±è´¥: {e}")
    
    def crawl_jjckb(self):
        """çˆ¬å–ç»æµå‚è€ƒæŠ¥è¦é—»"""
        url = "http://jjckb.xinhuanet.com/yw.htm"
        print(f"\nğŸ” çˆ¬å–: ç»æµå‚è€ƒæŠ¥è¦é—»")
        
        try:
            self.driver.get(url)
            time.sleep(3)
            
            news_items = self.driver.find_elements(By.CSS_SELECTOR, "ul.news-list li a, div.news-list a")
            
            for item in news_items[:30]:
                try:
                    title = item.text.strip()
                    href = item.get_attribute('href')
                    
                    if title and len(title) > 10 and self.match_keywords(title):
                        self.results.append({
                            'title': title,
                            'url': href,
                            'source': 'ç»æµå‚è€ƒæŠ¥',
                            'date': datetime.now().strftime('%Y-%m-%d')
                        })
                except:
                    continue
            
            print(f"  âœ“ æ‰¾åˆ° {len([r for r in self.results if r['source']=='ç»æµå‚è€ƒæŠ¥'])} æ¡åŒ¹é…æ–°é—»")
        except Exception as e:
            print(f"  âœ— çˆ¬å–å¤±è´¥: {e}")
    
    def fetch_article_content(self, url):
        """
        çˆ¬å–æ–‡ç« å†…å®¹
        
        å‚æ•°:
            url: æ–‡ç« URL
        
        è¿”å›:
            æ–‡ç« å†…å®¹å­—ç¬¦ä¸²
        """
        try:
            self.driver.get(url)
            time.sleep(2)
            
            # å°è¯•å¤šç§å¸¸è§çš„æ–‡ç« å†…å®¹é€‰æ‹©å™¨
            content_selectors = [
                'div.article-content',
                'div.content',
                'div.news-content',
                'div.detail-content',
                'article',
                'div#content',
                'div.main-content'
            ]
            
            for selector in content_selectors:
                try:
                    content_elem = self.driver.find_element(By.CSS_SELECTOR, selector)
                    content = content_elem.text.strip()
                    if content and len(content) > 100:  # ç¡®ä¿å†…å®¹è¶³å¤Ÿé•¿
                        return content
                except:
                    continue
            
            return ""
        except Exception as e:
            print(f"    âš ï¸ å†…å®¹çˆ¬å–å¤±è´¥: {e}")
            return ""
    
    def crawl_all_sources(self, fetch_content=False):
        """
        æ‰¹é‡çˆ¬å–æ‰€æœ‰é…ç½®çš„æ–°é—»æº
        
        å‚æ•°:
            fetch_content: æ˜¯å¦çˆ¬å–æ–‡ç« å†…å®¹
        """
        print(f"\n{'='*60}")
        print(f"å¼€å§‹çˆ¬å– {self.config['sectors'][self.sector]['name']} æ¿å—")
        print(f"{'='*60}")
        
        # æ ¹æ®æ¿å—é€‰æ‹©çˆ¬å–æ–¹æ³•
        if self.sector == 'education':
            self.crawl_mohrss()
        elif self.sector == 'healthcare':
            self.crawl_cs_com_cn()
            self.crawl_cls_cn()
            self.crawl_guandian_cn()
            self.crawl_jjckb()
        
        # å¦‚æœéœ€è¦çˆ¬å–æ–‡ç« å†…å®¹
        if fetch_content and self.results:
            print(f"\nğŸ“„ å¼€å§‹çˆ¬å–æ–‡ç« å†…å®¹...")
            for i, news in enumerate(self.results, 1):
                print(f"  {i}/{len(self.results)}: {news['title'][:30]}...")
                content = self.fetch_article_content(news['url'])
                news['content'] = content
                time.sleep(1)  # é¿å…è¯·æ±‚è¿‡å¿«
    
    def save_results(self):
        """ä¿å­˜ç»“æœ"""
        if not self.results:
            print("\nâš ï¸  æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ–°é—»")
            return
        
        script_dir = os.path.dirname(os.path.abspath(__file__))
        data_dir = os.path.join(script_dir, '../data/raw')
        os.makedirs(data_dir, exist_ok=True)
        
        date_str = datetime.now().strftime('%Y%m%d')
        filename = os.path.join(data_dir, f"{self.sector}_advanced_{date_str}.json")
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ å·²ä¿å­˜åˆ°: {filename}")
        print(f"ğŸ“Š å…±ä¿å­˜: {len(self.results)} æ¡æ–°é—»")
    
    def close(self):
        """å…³é—­æµè§ˆå™¨"""
        self.driver.quit()


def main():
    import argparse
    parser = argparse.ArgumentParser(description='é«˜çº§æ–°é—»çˆ¬è™«ï¼ˆæ”¯æŒJSæ¸²æŸ“ï¼‰')
    parser.add_argument('--sector', required=True, 
                       choices=['education', 'healthcare'],
                       help='æ¿å—')
    parser.add_argument('--content', action='store_true',
                       help='æ˜¯å¦çˆ¬å–æ–‡ç« å†…å®¹')
    args = parser.parse_args()
    
    crawler = AdvancedNewsCrawler(args.sector)
    
    try:
        crawler.crawl_all_sources(fetch_content=args.content)
        crawler.save_results()
    finally:
        crawler.close()


if __name__ == '__main__':
    main()

