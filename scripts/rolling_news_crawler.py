#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ»šåŠ¨æ–°é—»çˆ¬è™« - æ”¯æŒå…³é”®è¯è¿‡æ»¤å’Œç¿»é¡µ
"""

import json
import os
from datetime import datetime
import requests
from bs4 import BeautifulSoup
import time
import re

class RollingNewsCrawler:
    """æ»šåŠ¨æ–°é—»çˆ¬è™« - å…³é”®è¯è¿‡æ»¤ç‰ˆ"""
    
    def __init__(self, sector):
        self.sector = sector
        self.config = self.load_config()
        self.results = []
        self.session = requests.Session()
        
        # è®¾ç½®è¯·æ±‚å¤´
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
        })
        
        # è·å–å…³é”®è¯
        self.keywords = self.config['sectors'][sector]['keywords']
    
    def load_config(self):
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        script_dir = os.path.dirname(os.path.abspath(__file__))
        config_path = os.path.join(script_dir, '../references/config.json')
        with open(config_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    
    def match_keywords(self, title):
        """æ£€æŸ¥æ ‡é¢˜æ˜¯å¦åŒ…å«å…³é”®è¯"""
        for keyword in self.keywords:
            if keyword in title:
                return True
        return False
    
    def crawl_rolling_news(self, url, max_pages=3):
        """
        çˆ¬å–æ»šåŠ¨æ–°é—»
        
        å‚æ•°:
            url: æ–°é—»åˆ—è¡¨é¡µURL
            max_pages: æœ€å¤§ç¿»é¡µæ•°
        """
        print(f"\nğŸ” çˆ¬å–æ»šåŠ¨æ–°é—»: {url}")
        
        for page in range(1, max_pages + 1):
            print(f"  ğŸ“„ ç¬¬ {page} é¡µ...")
            
            # æ„é€ ç¿»é¡µURL
            page_url = self.build_page_url(url, page)
            
            try:
                response = self.session.get(page_url, timeout=10)
                
                # æ ¹æ®ç½‘ç«™è®¾ç½®ç¼–ç 
                if 'people.com.cn' in page_url:
                    response.encoding = 'gb2312'
                else:
                    response.encoding = 'utf-8'
                
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # æŸ¥æ‰¾æ–°é—»åˆ—è¡¨
                news_items = self.extract_news_list(soup, url)
                
                # è¿‡æ»¤å…³é”®è¯
                matched_count = 0
                for item in news_items:
                    if self.match_keywords(item['title']):
                        self.results.append(item)
                        matched_count += 1
                
                print(f"     âœ“ æ‰¾åˆ° {len(news_items)} æ¡æ–°é—»ï¼ŒåŒ¹é… {matched_count} æ¡")
                
                time.sleep(1)  # ç¤¼è²Œå»¶è¿Ÿ
                
            except Exception as e:
                print(f"     âœ— çˆ¬å–å¤±è´¥: {e}")
                break
        
        # å»é‡
        self.deduplicate()
    
    def build_page_url(self, base_url, page):
        """æ„é€ ç¿»é¡µURL"""
        # ä¸åŒç½‘ç«™çš„ç¿»é¡µè§„åˆ™ä¸åŒ
        if 'people.com.cn' in base_url:
            # äººæ°‘ç½‘: index.html -> index2.html, index3.html
            if page == 1:
                return base_url
            else:
                return base_url.replace('index.html', f'index{page}.html')
        elif 'sina.com.cn' in base_url:
            # æ–°æµª: ä½¿ç”¨pageå‚æ•°
            return base_url.replace('page=1', f'page={page}')
        else:
            # é»˜è®¤ï¼šå°è¯•æ·»åŠ pageå‚æ•°
            separator = '&' if '?' in base_url else '?'
            return f"{base_url}{separator}page={page}"
    
    def extract_news_list(self, soup, base_url):
        """ä»HTMLä¸­æå–æ–°é—»åˆ—è¡¨"""
        news_items = []
        
        # é’ˆå¯¹ä¸åŒç½‘ç«™ä½¿ç”¨ä¸åŒçš„ç­–ç•¥
        if 'people.com.cn' in base_url:
            # äººæ°‘ç½‘ï¼šul.list_16 li a
            news_items = self._extract_people_news(soup, base_url)
        elif 'ce.cn' in base_url:
            # ä¸­å›½ç»æµç½‘ï¼šæŸ¥æ‰¾åŒ…å«æ—¥æœŸçš„é“¾æ¥
            news_items = self._extract_ce_news(soup, base_url)
        elif 'stdaily.com' in base_url:
            # ä¸­å›½ç§‘æŠ€ç½‘
            news_items = self._extract_stdaily_news(soup, base_url)
        elif 'tibet.cn' in base_url:
            # ä¸­å›½è¥¿è—ç½‘
            news_items = self._extract_tibet_news(soup, base_url)
        else:
            # é€šç”¨æ–¹æ³•
            news_items = self._extract_generic_news(soup, base_url)
        
        return news_items
    
    def _extract_people_news(self, soup, base_url):
        """æå–äººæ°‘ç½‘æ–°é—»"""
        news_items = []
        uls = soup.find_all('ul', class_='list_16')
        
        for ul in uls:
            links = ul.find_all('a', href=True)
            for link in links:
                title = link.get_text().strip()
                href = link.get('href', '')
                
                if not title or len(title) < 10:
                    continue
                
                # è¡¥å…¨URL
                if href.startswith('/'):
                    from urllib.parse import urlparse
                    parsed = urlparse(base_url)
                    href = f"{parsed.scheme}://{parsed.netloc}{href}"
                
                news_items.append({
                    'title': title,
                    'url': href,
                    'source': 'äººæ°‘ç½‘æ»šåŠ¨',
                    'date': datetime.now().strftime('%Y-%m-%d')
                })
        
        return news_items
    
    def _extract_ce_news(self, soup, base_url):
        """æå–ä¸­å›½ç»æµç½‘æ–°é—»"""
        news_items = []
        all_links = soup.find_all('a', href=True)
        
        for link in all_links:
            title = link.get_text().strip()
            href = link.get('href', '')
            
            # è¿‡æ»¤ï¼šæ ‡é¢˜é•¿åº¦åˆé€‚ï¼Œä¸”é“¾æ¥åŒ…å«æ—¥æœŸ
            if title and 15 < len(title) < 100 and '/202' in href:
                # è¡¥å…¨URL
                if href.startswith('./'):
                    href = base_url + href[2:]
                elif href.startswith('/'):
                    href = 'http://www.ce.cn' + href
                
                news_items.append({
                    'title': title,
                    'url': href,
                    'source': 'ä¸­å›½ç»æµç½‘',
                    'date': datetime.now().strftime('%Y-%m-%d')
                })
        
        return news_items
    
    def _extract_stdaily_news(self, soup, base_url):
        """æå–ä¸­å›½ç§‘æŠ€ç½‘æ–°é—»"""
        news_items = []
        all_links = soup.find_all('a', href=True)
        
        for link in all_links:
            title = link.get_text().strip()
            href = link.get('href', '')
            
            # è¿‡æ»¤ï¼šæ ‡é¢˜é•¿åº¦åˆé€‚ï¼Œé“¾æ¥åŒ…å«æ—¥æœŸ
            if title and 15 < len(title) < 100 and '/202' in href:
                # è¡¥å…¨URL
                if not href.startswith('http'):
                    href = 'https://www.stdaily.com' + href if href.startswith('/') else href
                
                news_items.append({
                    'title': title,
                    'url': href,
                    'source': 'ä¸­å›½ç§‘æŠ€ç½‘',
                    'date': datetime.now().strftime('%Y-%m-%d')
                })
        
        return news_items
    
    def _extract_tibet_news(self, soup, base_url):
        """æå–ä¸­å›½è¥¿è—ç½‘æ–°é—»"""
        news_items = []
        all_links = soup.find_all('a', href=True)
        
        for link in all_links:
            title = link.get_text().strip()
            href = link.get('href', '')
            
            # è¿‡æ»¤ï¼šæ ‡é¢˜é•¿åº¦åˆé€‚
            if title and 15 < len(title) < 100:
                # è¡¥å…¨URL
                if href.startswith('./'):
                    href = base_url + href[2:]
                elif href.startswith('/'):
                    href = 'http://www.tibet.cn' + href
                
                news_items.append({
                    'title': title,
                    'url': href,
                    'source': 'ä¸­å›½è¥¿è—ç½‘',
                    'date': datetime.now().strftime('%Y-%m-%d')
                })
        
        return news_items
    
    def _extract_generic_news(self, soup, base_url):
        """é€šç”¨æ–°é—»æå–æ–¹æ³•"""
        news_items = []
        
        # å°è¯•å¤šç§é€‰æ‹©å™¨
        selectors = [
            'ul.news_list li a',
            'div.news-list li a',
            'ul.list li a',
            'div.list-item a',
        ]
        
        for selector in selectors:
            links = soup.select(selector)
            if links and len(links) > 5:
                for link in links[:50]:
                    title = link.get_text().strip()
                    href = link.get('href', '')
                    
                    if not title or len(title) < 10:
                        continue
                    
                    # è¡¥å…¨URL
                    if href.startswith('/'):
                        from urllib.parse import urlparse
                        parsed = urlparse(base_url)
                        href = f"{parsed.scheme}://{parsed.netloc}{href}"
                    elif not href.startswith('http'):
                        continue
                    
                    news_items.append({
                        'title': title,
                        'url': href,
                        'source': 'æ»šåŠ¨æ–°é—»',
                        'date': datetime.now().strftime('%Y-%m-%d')
                    })
                break
        
        return news_items
    
    def deduplicate(self):
        """å»é‡åŠŸèƒ½"""
        seen_titles = set()
        seen_urls = set()
        unique_results = []
        
        for news in self.results:
            title = news['title']
            url = news['url']
            
            # æ ¹æ®æ ‡é¢˜å’ŒURLå»é‡
            if title not in seen_titles and url not in seen_urls:
                seen_titles.add(title)
                seen_urls.add(url)
                unique_results.append(news)
        
        removed = len(self.results) - len(unique_results)
        if removed > 0:
            print(f"\nğŸ”„ å»é‡: ç§»é™¤ {removed} æ¡é‡å¤æ–°é—»")
        
        self.results = unique_results
    
    def save_results(self):
        """ä¿å­˜ç»“æœï¼ˆè¿½åŠ æ¨¡å¼ï¼‰"""
        script_dir = os.path.dirname(os.path.abspath(__file__))
        data_dir = os.path.join(script_dir, '../data/raw')
        os.makedirs(data_dir, exist_ok=True)
        
        date_str = datetime.now().strftime('%Y%m%d')
        filename = os.path.join(data_dir, f"{self.sector}_rolling_{date_str}.json")
        
        # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œå…ˆåŠ è½½ç°æœ‰æ•°æ®
        existing_data = []
        if os.path.exists(filename):
            try:
                with open(filename, 'r', encoding='utf-8') as f:
                    existing_data = json.load(f)
            except:
                existing_data = []
        
        # åˆå¹¶æ•°æ®
        all_data = existing_data + self.results
        
        # å»é‡
        seen_urls = set()
        unique_data = []
        for item in all_data:
            url = item.get('url', '')
            if url and url not in seen_urls:
                seen_urls.add(url)
                unique_data.append(item)
        
        # ä¿å­˜
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(unique_data, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ å·²ä¿å­˜åˆ°: {filename}")
        print(f"ğŸ“Š å…±ä¿å­˜: {len(unique_data)} æ¡æ–°é—»ï¼ˆæœ¬æ¬¡æ–°å¢: {len(self.results)} æ¡ï¼‰")


def main():
    import argparse
    parser = argparse.ArgumentParser(description='æ»šåŠ¨æ–°é—»çˆ¬è™«')
    parser.add_argument('--sector', required=True, 
                       choices=['education', 'healthcare'],
                       help='æ¿å—')
    parser.add_argument('--url', help='æ»šåŠ¨æ–°é—»URLï¼ˆå•ä¸ªçˆ¬å–ï¼‰')
    parser.add_argument('--all', action='store_true', help='çˆ¬å–é…ç½®æ–‡ä»¶ä¸­çš„æ‰€æœ‰æ–°é—»æº')
    parser.add_argument('--pages', type=int, default=10, help='ç¿»é¡µæ•°')
    args = parser.parse_args()
    
    crawler = RollingNewsCrawler(args.sector)
    
    if args.all:
        # æ‰¹é‡çˆ¬å–é…ç½®æ–‡ä»¶ä¸­çš„æ‰€æœ‰æ–°é—»æº
        print(f"\n{'='*60}")
        print(f"æ‰¹é‡çˆ¬å– {crawler.config['sectors'][args.sector]['name']} æ¿å—çš„æ‰€æœ‰æ–°é—»æº")
        print(f"{'='*60}")
        
        news_sources = crawler.config['sectors'][args.sector]['news_sources']
        for source in news_sources:
            print(f"\nğŸ“° çˆ¬å–: {source['name']}")
            crawler.crawl_rolling_news(source['url'], args.pages)
            time.sleep(2)  # é¿å…è¯·æ±‚è¿‡å¿«
    elif args.url:
        # å•ä¸ªURLçˆ¬å–
        crawler.crawl_rolling_news(args.url, args.pages)
    else:
        parser.error('è¯·æŒ‡å®š --url æˆ– --all å‚æ•°')
    
    crawler.save_results()


if __name__ == '__main__':
    main()

