#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ–°é—»çˆ¬è™«ç³»ç»Ÿ - å®Œæ•´å®ç°ç‰ˆ
æ”¯æŒå¤šç§çˆ¬å–ç­–ç•¥ï¼Œåº”å¯¹ä¸åŒç½‘ç«™çš„åçˆ¬è™«æœºåˆ¶
"""

import json
import os
import sys
from datetime import datetime, timedelta
import requests
from bs4 import BeautifulSoup
import time
import re

class NewsCrawler:
    """æ–°é—»çˆ¬è™«ä¸»ç±»"""
    
    def __init__(self, sector, count=10):
        self.sector = sector
        self.count = count
        self.config = self.load_config()
        self.results = []
        self.session = requests.Session()
        
        # è®¾ç½®è¯·æ±‚å¤´ï¼Œæ¨¡æ‹Ÿæµè§ˆå™¨
        self.session.headers.update({
            'User-Agent': self.config['crawler_settings']['user_agent'],
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
        })
    
    def load_config(self):
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        script_dir = os.path.dirname(os.path.abspath(__file__))
        config_path = os.path.join(script_dir, '../references/config.json')
        with open(config_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    
    def crawl(self):
        """ä¸»çˆ¬å–æ–¹æ³•"""
        print(f"\n{'='*60}")
        print(f"ğŸ•·ï¸  å¼€å§‹çˆ¬å– {self.config['sectors'][self.sector]['name']} æ¿å—æ–°é—»")
        print(f"ğŸ“Š ç›®æ ‡æ•°é‡: {self.count} æ¡")
        print(f"{'='*60}\n")
        
        # ç­–ç•¥1: çˆ¬å–ä¸“ä¸šç½‘ç«™ï¼ˆåŒ»ç–—å¥åº·æ¿å—ï¼‰
        if self.sector == 'healthcare' and 'ä¸“ä¸šç½‘ç«™' in self.config['sectors'][self.sector]:
            print("ğŸ¥ ç­–ç•¥1: ä¸“ä¸šç½‘ç«™çˆ¬å–...")
            self.crawl_professional_sites()
        
        # ç­–ç•¥2: é€šç”¨æ–°é—»ç½‘ç«™çˆ¬å–
        print("\nğŸ“° ç­–ç•¥2: é€šç”¨æ–°é—»ç½‘ç«™...")
        self.crawl_general_news_sites()
        
        # å»é‡
        self.deduplicate()
        
        print(f"\nâœ… çˆ¬å–å®Œæˆï¼å…±è·å– {len(self.results)} æ¡æ–°é—»")
        return self.results
    
    def crawl_professional_sites(self):
        """
        çˆ¬å–ä¸“ä¸šç½‘ç«™ï¼ˆåŒ»è¯ç½‘ã€å¥åº·æŠ¥ï¼‰
        
        çˆ¬è™«çŸ¥è¯†ç‚¹ï¼š
        1. ä¸åŒç½‘ç«™çš„HTMLç»“æ„ä¸åŒï¼Œéœ€è¦é’ˆå¯¹æ€§è§£æ
        2. æœ‰äº›ç½‘ç«™æœ‰åçˆ¬è™«æœºåˆ¶ï¼Œéœ€è¦ç‰¹æ®Šå¤„ç†
        """
        sites = self.config['sectors'][self.sector].get('ä¸“ä¸šç½‘ç«™', [])
        
        for site in sites:
            print(f"  ğŸ“ {site['name']}: {site['url']}")
            
            if site['name'] == 'å¥åº·æŠ¥':
                self.crawl_jkb()
            elif site['name'] == 'åŒ»è¯ç½‘':
                self.crawl_pharmnet()
            
            time.sleep(2)  # ç¤¼è²Œæ€§å»¶è¿Ÿï¼Œé¿å…ç»™æœåŠ¡å™¨å‹åŠ›
    
    def crawl_jkb(self):
        """
        çˆ¬å–å¥åº·æŠ¥ç½‘ç«™
        
        æ­¥éª¤è®²è§£ï¼š
        1. è®¿é—®å¥åº·æŠ¥é¦–é¡µ
        2. æ‰¾åˆ°æ–°é—»åˆ—è¡¨åŒºåŸŸ
        3. æå–æ¯æ¡æ–°é—»çš„æ ‡é¢˜ã€é“¾æ¥ã€æ—¶é—´
        """
        try:
            url = 'https://www.jkb.com.cn/'
            response = self.session.get(url, timeout=10)
            response.encoding = 'utf-8'
            
            if response.status_code != 200:
                print(f"    âœ— è®¿é—®å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
                return
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # æŸ¥æ‰¾æ–°é—»åˆ—è¡¨ï¼ˆéœ€è¦æ ¹æ®å®é™…HTMLç»“æ„è°ƒæ•´ï¼‰
            # è¿™é‡Œæ˜¯é€šç”¨çš„æŸ¥æ‰¾æ–¹æ³•
            news_links = soup.find_all('a', href=re.compile(r'/news/|/article/'))
            
            count = 0
            for link in news_links[:5]:  # é™åˆ¶5æ¡
                title = link.get_text().strip()
                href = link.get('href', '')
                
                if not title or len(title) < 10:
                    continue
                
                # è¡¥å…¨URL
                if href.startswith('/'):
                    href = 'https://www.jkb.com.cn' + href
                
                news_data = {
                    'title': title,
                    'url': href,
                    'source': 'å¥åº·æŠ¥',
                    'date': datetime.now().strftime('%Y-%m-%d'),
                    'content': ''
                }
                
                self.results.append(news_data)
                count += 1
            
            print(f"    âœ“ è·å– {count} æ¡æ–°é—»")
            
        except Exception as e:
            print(f"    âœ— çˆ¬å–å¤±è´¥: {e}")
    
    def crawl_pharmnet(self):
        """
        çˆ¬å–åŒ»è¯ç½‘
        
        æ³¨æ„ï¼šåŒ»è¯ç½‘æœ‰åçˆ¬è™«æœºåˆ¶ï¼ˆå®‰å…¨æ£€æŸ¥ï¼‰
        è§£å†³æ–¹æ¡ˆï¼š
        1. ä½¿ç”¨æ›´çœŸå®çš„è¯·æ±‚å¤´
        2. æ·»åŠ  Referer
        3. å¦‚æœè¿˜æ˜¯ä¸è¡Œï¼Œå¯ä»¥å°è¯•ä½¿ç”¨ Seleniumï¼ˆæµè§ˆå™¨è‡ªåŠ¨åŒ–ï¼‰
        """
        try:
            url = 'https://www.pharmnet.com.cn/news/'
            
            # æ·»åŠ æ›´å¤šè¯·æ±‚å¤´æ¥ç»•è¿‡åçˆ¬è™«
            headers = self.session.headers.copy()
            headers['Referer'] = 'https://www.pharmnet.com.cn/'
            
            response = self.session.get(url, headers=headers, timeout=10)
            
            if 'å®‰å…¨æ£€æŸ¥' in response.text or response.status_code != 200:
                print(f"    âš ï¸  åŒ»è¯ç½‘æœ‰åçˆ¬è™«æœºåˆ¶ï¼Œè·³è¿‡")
                print(f"    ğŸ’¡ å»ºè®®ï¼šä½¿ç”¨ RSS è®¢é˜…æˆ– API æ–¹å¼")
                return
            
            # å¦‚æœèƒ½è®¿é—®ï¼Œè§£ææ–°é—»åˆ—è¡¨
            soup = BeautifulSoup(response.text, 'html.parser')
            # ... è§£æé€»è¾‘
            
        except Exception as e:
            print(f"    âœ— çˆ¬å–å¤±è´¥: {e}")
    
    def crawl_general_news_sites(self):
        """
        çˆ¬å–é€šç”¨æ–°é—»ç½‘ç«™ï¼ˆæ–°åç½‘ã€äººæ°‘ç½‘ç­‰ï¼‰
        
        ç­–ç•¥ï¼šä½¿ç”¨æœç´¢å¼•æ“æˆ–RSSè®¢é˜…
        è¿™é‡Œæä¾›ä¸€ä¸ªæ¡†æ¶ï¼Œå®é™…éœ€è¦é…åˆå…¶ä»–å·¥å…·
        """
        print("    ğŸ’¡ é€šç”¨æ–°é—»ç½‘ç«™å»ºè®®ä½¿ç”¨ä»¥ä¸‹æ–¹å¼ï¼š")
        print("       1. RSSè®¢é˜…ï¼ˆæœ€ç¨³å®šï¼‰")
        print("       2. æœç´¢å¼•æ“APIï¼ˆweb_searchï¼‰")
        print("       3. æ–°é—»èšåˆAPI")
        print("    âš ï¸  å½“å‰ç‰ˆæœ¬æš‚æœªå®ç°ï¼Œè¯·ä½¿ç”¨ web_search è¡¥å……")
    
    def deduplicate(self):
        """å»é‡åŠŸèƒ½ - æ ¹æ®æ ‡é¢˜å»é™¤é‡å¤çš„æ–°é—»"""
        seen_titles = set()
        unique_results = []
        
        for news in self.results:
            title = news['title']
            if title not in seen_titles:
                seen_titles.add(title)
                unique_results.append(news)
        
        removed = len(self.results) - len(unique_results)
        if removed > 0:
            print(f"\nğŸ”„ å»é‡: ç§»é™¤ {removed} æ¡é‡å¤æ–°é—»")
        
        self.results = unique_results
    
    def save_results(self):
        """ä¿å­˜çˆ¬å–ç»“æœåˆ°JSONæ–‡ä»¶"""
        script_dir = os.path.dirname(os.path.abspath(__file__))
        data_dir = os.path.join(script_dir, '../data/raw')
        os.makedirs(data_dir, exist_ok=True)
        
        date_str = datetime.now().strftime('%Y%m%d')
        filename = os.path.join(data_dir, f"{self.sector}_{date_str}.json")
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ å·²ä¿å­˜åˆ°: {filename}")
        print(f"ğŸ“Š å…±ä¿å­˜: {len(self.results)} æ¡æ–°é—»")


def main():
    import argparse
    parser = argparse.ArgumentParser(description='æ–°é—»çˆ¬è™«ç³»ç»Ÿ v2')
    parser.add_argument('--sector', required=True, 
                       choices=['education', 'healthcare'],
                       help='æ¿å—: education(æ•™è‚²äººæ‰) æˆ– healthcare(åŒ»ç–—å¥åº·)')
    parser.add_argument('--count', type=int, default=10,
                       help='ç›®æ ‡æ•°é‡ (é»˜è®¤: 10)')
    args = parser.parse_args()
    
    crawler = NewsCrawler(args.sector, args.count)
    crawler.crawl()
    crawler.save_results()


if __name__ == '__main__':
    main()
